[
  {
    "objectID": "Work in Progress/useful_sources.html",
    "href": "Work in Progress/useful_sources.html",
    "title": "Gigi Ji Yeon Sung's Workspace",
    "section": "",
    "text": "Various functions app developed by Dr. Qiusheng Wu, Assistant Professor of Geographic Information Science at the University of Tennessee, Knoxville.\nhttps://huggingface.co/spaces/giswqs/Streamlit"
  },
  {
    "objectID": "Work in Progress/mapping_uncertainty.html",
    "href": "Work in Progress/mapping_uncertainty.html",
    "title": "Mapping Uncertainty in ACS Data",
    "section": "",
    "text": "Task 1: Obtaining Census Enumeration Unit GIS Boundary Data\nTask 2: Obtaining Demographic and Socioeconomic Data Tables from the US Census Bureau\nTask 3: Calculate Coefficients of Variation for ACS Data\nTask 4: Categorize Census Tracts According to Data Quality as Indicated by CVs\nTask 5: Join the File to the Florida Tracts Geometry\nTask 6: Task 6: Merge the demographic data with geometries\nTask 7: Map the Poverty Rate Data and Include Data Quality as Indicated by CVs",
    "crumbs": [
      "Work in Progress",
      "Mapping Uncertainty in ACS Data"
    ]
  },
  {
    "objectID": "Work in Progress/mapping_uncertainty.html#task-1-obtaining-census-enumeration-unit-gis-boundary-data",
    "href": "Work in Progress/mapping_uncertainty.html#task-1-obtaining-census-enumeration-unit-gis-boundary-data",
    "title": "Mapping Uncertainty in ACS Data",
    "section": "Task 1: Obtaining Census Enumeration Unit GIS Boundary Data ",
    "text": "Task 1: Obtaining Census Enumeration Unit GIS Boundary Data",
    "crumbs": [
      "Work in Progress",
      "Mapping Uncertainty in ACS Data"
    ]
  },
  {
    "objectID": "Work in Progress/mapping_uncertainty.html#task-2-obtaining-demographic-and-socioeconomic-data-tables-from-the-us-census-bureau",
    "href": "Work in Progress/mapping_uncertainty.html#task-2-obtaining-demographic-and-socioeconomic-data-tables-from-the-us-census-bureau",
    "title": "Mapping Uncertainty in ACS Data",
    "section": "Task 2: Obtaining Demographic and Socioeconomic Data Tables from the US Census Bureau ",
    "text": "Task 2: Obtaining Demographic and Socioeconomic Data Tables from the US Census Bureau \n\n# Data analysis\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\n\n# APIs\nimport requests\n\n# Plotting\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nimport hvplot.pandas\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\npd.options.display.max_columns = 999\npd.options.display.max_colwidth = None\n\n‘cenpy’ - “Explore and download data from Census APIs”\nDocumentation: https://cenpy-devs.github.io/cenpy/\n\nStep 1: Identify what dataset we want\n\n# First step: import cenpy\nimport cenpy\nimport matplotlib.pyplot as plt\n\n\n### Step 1: Identify what dataset we want\navailable = cenpy.explorer.available()\n\navailable.head()\n\n/Users/gigisung/anaconda3/envs/eda/lib/python3.12/site-packages/cenpy/explorer.py:70: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  listcols = raw_table.applymap(lambda x: isinstance(x, list)).any()\n\n\n\n\n\n\n\n\n\n\nc_isTimeseries\nc_isMicrodata\npublisher\ntemporal\nspatial\nprogramCode\nmodified\nkeyword\ncontactPoint\ndistribution\ndescription\nbureauCode\naccessLevel\ntitle\nc_isAvailable\nc_isCube\nc_isAggregate\nc_dataset\nvintage\n\n\n\n\nABSCB2017\nNaN\nNaN\nU.S. Census Bureau\n2017/2017\nUS\n006:007\n2020-04-30 00:00:00.0\n(census,)\n{'fn': 'ASE Staff', 'hasEmail': 'mailto:erd.annual.survey.of.entrepreneurs@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2017/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, the survey measures research and development (for microbusinesses), new business topics such as innovation and technology, as well as other business characteristics. The U.S. Census Bureau and the National Center conduct the ABS jointly for Science and Engineering Statistics within the National Science Foundation. The ABS replaces the five-year Survey of Business Owners (SBO) for employer businesses, the Annual Survey of Entrepreneurs (ASE), the Business R&D and Innovation for Microbusinesses survey (BRDI-M), and the innovation section of the Business R&D and Innovation Survey (BRDI-S). https://www.census.gov/programs-surveys/abs.html\n006:07\npublic\nEconomic Surveys: Annual Business Survey: Characteristics of Businesses\nTrue\nNaN\nTrue\n(abscb,)\n2017.0\n\n\nABSCB2018\nNaN\nNaN\nU.S. Census Bureau\n2018/2018\nUS\n006:007\n2020-10-26 00:00:00.0\n(census,)\n{'fn': 'ASE Staff', 'hasEmail': 'mailto:Erd.annual.survey.of.entrepreneurs@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2018/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, the survey measures research and development (for microbusinesses), new business topics such as innovation and technology, as well as other business characteristics. The U.S. Census Bureau and the National Center conduct the ABS jointly for Science and Engineering Statistics within the National Science Foundation. The ABS replaces the five-year Survey of Business Owners (SBO) for employer businesses, the Annual Survey of Entrepreneurs (ASE), the Business R&D and Innovation for Microbusinesses survey (BRDI-M), and the innovation section of the Business R&D and Innovation Survey (BRDI-S). https://www.census.gov/programs-surveys/abs.html\n006:07\npublic\nEconomic Surveys: Annual Business Survey: Characteristics of Businesses\nTrue\nNaN\nTrue\n(abscb,)\n2018.0\n\n\nABSCB2019\nNaN\nNaN\nU.S. Census Bureau\n2019/2019\nUS\n006:007\n2021-08-17 00:00:00.0\n(census,)\n{'fn': 'ASE Staff', 'hasEmail': 'mailto:ERD.annual.survey.of.entrepreneurs@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2019/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, the survey measures research and development (for microbusinesses), new business topics such as innovation and technology, as well as other business characteristics. The U.S. Census Bureau and the National Center conduct the ABS jointly for Science and Engineering Statistics within the National Science Foundation. The ABS replaces the five-year Survey of Business Owners (SBO) for employer businesses, the Annual Survey of Entrepreneurs (ASE), the Business R&D and Innovation for Microbusinesses survey (BRDI-M), and the innovation section of the Business R&D and Innovation Survey (BRDI-S). https://www.census.gov/programs-surveys/abs.html\n006:07\npublic\nEconomic Surveys: Annual Business Survey: Characteristics of Businesses\nTrue\nNaN\nTrue\n(abscb,)\n2019.0\n\n\nABSCB2020\nNaN\nNaN\nU.S. Census Bureau\n2020/2020\nUS\n006:007\n2022-08-03 00:00:00.0\n(census,)\n{'fn': 'ASE Staff', 'hasEmail': 'mailto:ERD.annual.survey.of.entrepreneurs@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2020/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, the survey measures research and development (for microbusinesses), new business topics such as innovation and technology, as well as other business characteristics. The U.S. Census Bureau and the National Center conduct the ABS jointly for Science and Engineering Statistics within the National Science Foundation. The ABS replaces the five-year Survey of Business Owners (SBO) for employer businesses, the Annual Survey of Entrepreneurs (ASE), the Business R&D and Innovation for Microbusinesses survey (BRDI-M), and the innovation section of the Business R&D and Innovation Survey (BRDI-S). https://www.census.gov/programs-surveys/abs.html\n006:07\npublic\nEconomic Surveys: Annual Business Survey: Characteristics of Businesses\nTrue\nNaN\nTrue\n(abscb,)\n2020.0\n\n\nABSCB2021\nNaN\nNaN\nU.S. Census Bureau\n2021/2021\nUnited States\n006:007\n2023-07-24 10:30:52.0\n(census,)\n{'fn': 'ABS Staff', 'hasEmail': 'mailto:adep.annual.business.survey@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2021/abscb', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe Annual Business Survey (ABS) provides information on selected economic and demographic characteristics for businesses and business owners by sex, ethnicity, race, and veteran status. Further, the survey measures research and development (for microbusinesses), new business topics such as innovation and technology, as well as other business characteristics. The U.S. Census Bureau and the National Center conduct the ABS jointly for Science and Engineering Statistics within the National Science Foundation. The ABS replaces the five-year Survey of Business Owners (SBO) for employer businesses, the Annual Survey of Entrepreneurs (ASE), the Business R&D and Innovation for Microbusinesses survey (BRDI-M), and the innovation section of the Business R&D and Innovation Survey (BRDI-S). https://www.census.gov/programs-surveys/abs.html\n006:07\npublic\nEconomic Surveys: Annual Business Survey: Characteristics of Businesses\nTrue\nNaN\nTrue\n(abscb,)\n2021.0\n\n\n\n\n\n\n\n\n\n# Return a dataframe of all datasets that start with \"ACS\"\n# Axis=0 means to filter the index labels!\nacs = available.filter(regex=\"^ACS\", axis=0)\n\nacs.sort_values(by=\"temporal\")\n\n\n\n\n\n\n\n\n\nc_isTimeseries\nc_isMicrodata\npublisher\ntemporal\nspatial\nprogramCode\nmodified\nkeyword\ncontactPoint\ndistribution\ndescription\nbureauCode\naccessLevel\ntitle\nc_isAvailable\nc_isCube\nc_isAggregate\nc_dataset\nvintage\n\n\n\n\nACSSPP1Y2010\nNaN\nNaN\nU.S. Census Bureau\n2010/2010\nUnited States\n006:004\n2020-02-13 00:00:00.0\n(census,)\n{'fn': 'American Community Survey Office', 'hasEmail': 'mailto:acso.users.support@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2010/acs/acs1/spp', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nSelected Population Profiles provide broad social, economic, and housing profiles for a large number of race, ethnic, ancestry, and country/region of birth groups. The data are presented as population counts for the total population and various subgroups and percentages.\n006:07\npublic\nAmerican Community Survey: 1-Year Estimates: Selected Population Profiles 1-Year\nTrue\nTrue\nTrue\n(acs, acs1, spp)\n2010.0\n\n\nACSCD1132011\nNaN\nNaN\nU.S. Census Bureau\n2011/2011\nUnited States\n006:004\n2014-10-06\n(census,)\n{'fn': 'American Community Survey Office', 'hasEmail': 'mailto:acso.users.support@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2011/acs1/cd113', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe American Community Survey (ACS) is a nationwide survey designed to provide communities a fresh look at how they are changing. The ACS replaced the decennial census long form in 2010 and thereafter by collecting long form type information throughout the decade rather than only once every 10 years. Questionnaires are mailed to a sample of addresses to obtain information about households -- that is, about each person and the housing unit itself. The American Community Survey produces demographic, social, housing and economic estimates in the form of 1-year, 3-year and 5-year estimates based on population thresholds. The strength of the ACS is in estimating population and housing characteristics. The 3-year data provide key estimates for each of the topic areas covered by the ACS for the nation, all 50 states, the District of Columbia, Puerto Rico, every congressional district, every metropolitan area, and all counties and places with populations of 20,000 or more. Although the ACS produces population, demographic and housing unit estimates,it is the Census Bureau's Population Estimates Program that produces and disseminates the official estimates of the population for the nation, states, counties, cities and towns, and estimates of housing units for states and counties. For 2010 and other decennial census years, the Decennial Census provides the official counts of population and housing units.\n006:07\npublic\n2011 American Community Survey 1-Year Profiles for the 113th Congressional Districts\nTrue\nNaN\nTrue\n(acs1, cd113)\n2011.0\n\n\nACSLANG5Y2013\nNaN\nNaN\nU.S. Census Bureau\n2013/2013\nUnited States\n006:004\n2015-09-02\n(census,)\n{'fn': 'Education and Social Stratification Branch', 'hasEmail': 'mailto:dsd.ferrett@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2013/language', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThis data set uses the 2009-2013 American Community Survey to tabulate the number of speakers of languages spoken at home and the number of speakers of each language who speak English less than very well. These tabulations are available for the following geographies: nation; each of the 50 states, plus Washington, D.C. and Puerto Rico; counties with 100,000 or more total population and 25,000 or more speakers of languages other than English and Spanish; core-based statistical areas (metropolitan statistical areas and micropolitan statistical areas) with 100,000 or more total population and 25,000 or more speakers of languages other than English and Spanish.\n006:07\npublic\n2013 American Community Survey - Table Packages: Detailed Language Spoken in the U.S.\nTrue\nNaN\nTrue\n(language,)\n2013.0\n\n\nACSCD1152015\nNaN\nNaN\nU.S. Census Bureau\n2015/2015\nUnited States\n006:004\n2017-02-10\n(census,)\n{'fn': 'American Community Survey Office', 'hasEmail': 'mailto:acso.users.support@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2015/acs1/cd115', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe American Community Survey (ACS) is an ongoing survey that provides data every year -- giving communities the current information they need to plan investments and services. The ACS covers a broad range of topics about social, economic, demographic, and housing characteristics of the U.S. population. The 115th Congressional District Data Profiles are available for the nation, all 50 states, the District of Columbia, and Puerto Rico (at large). Data profiles contain broad social, economic, housing, and demographic information. The data are presented as population counts for over 1,000 distinct variables.\n006:07\npublic\n2015 American Community Survey 1-Year Data Profile 115th Congressional District\nTrue\nNaN\nTrue\n(acs1, cd115)\n2015.0\n\n\nACSEEO5Y2018\nNaN\nNaN\nU.S. Census Bureau\n2018/2018\nNaN\n006:004\n2021-07-28 00:00:00.0\n(census,)\n{'fn': 'American Community Survey Office', 'hasEmail': 'mailto:acso.users.support@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2018/acs/acs5/eeo', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nEqual Employment Opportunity Tabulation (5-year ACS data)\n006:07\npublic\nAmerican Community Survey: 5-Year Estimates: Equal Employment Opportunity 5-Year\nTrue\nTrue\nTrue\n(acs, acs5, eeo)\n2018.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nACSST5Y2015\nNaN\nNaN\nU.S. Census Bureau\nNaN\nNaN\n006:004\n2018-06-29 00:00:00.0\n(census,)\n{'fn': 'American Community Survey Office', 'hasEmail': 'mailto:acso.users.support@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2015/acs/acs5/subject', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe American Community Survey (ACS) is an ongoing survey that provides data every year -- giving communities the current information they need to plan investments and services. The ACS covers a broad range of topics about social, economic, demographic, and housing characteristics of the U.S. population. The subject tables include the following geographies: nation, all states (including DC and Puerto Rico), all metropolitan areas, all congressional districts, all counties, all places and all tracts. Subject tables provide an overview of the estimates available in a particular topic. The data are presented as both counts and percentages. There are over 66,000 variables in this dataset.\n006:07\npublic\nACS 5-Year Subject Tables\nTrue\nTrue\nTrue\n(acs, acs5, subject)\n2015.0\n\n\nACSST5Y2016\nNaN\nNaN\nU.S. Census Bureau\nNaN\nNaN\n006:004\n2018-06-29 00:00:00.0\n(census,)\n{'fn': 'American Community Survey Office', 'hasEmail': 'mailto:acso.users.support@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2016/acs/acs5/subject', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe American Community Survey (ACS) is an ongoing survey that provides data every year -- giving communities the current information they need to plan investments and services. The ACS covers a broad range of topics about social, economic, demographic, and housing characteristics of the U.S. population. The subject tables include the following geographies: nation, all states (including DC and Puerto Rico), all metropolitan areas, all congressional districts, all counties, all places and all tracts. Subject tables provide an overview of the estimates available in a particular topic. The data are presented as both counts and percentages. There are over 66,000 variables in this dataset.\n006:07\npublic\nACS 5-Year Subject Tables\nTrue\nTrue\nTrue\n(acs, acs5, subject)\n2016.0\n\n\nACSST5Y2017\nNaN\nNaN\nU.S. Census Bureau\nNaN\nNaN\n006:004\n2018-10-19 00:00:00.0\n(census,)\n{'fn': 'American Community Survey Office', 'hasEmail': 'mailto:acso.users.support@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2017/acs/acs5/subject', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe American Community Survey (ACS) is an ongoing survey that provides data every year -- giving communities the current information they need to plan investments and services. The ACS covers a broad range of topics about social, economic, demographic, and housing characteristics of the U.S. population. The subject tables include the following geographies: nation, all states (including DC and Puerto Rico), all metropolitan areas, all congressional districts, all counties, all places and all tracts. Subject tables provide an overview of the estimates available in a particular topic. The data are presented as both counts and percentages. There are over 66,000 variables in this dataset.\n006:07\npublic\nACS 5-Year Subject Tables\nTrue\nTrue\nTrue\n(acs, acs5, subject)\n2017.0\n\n\nACSST5Y2018\nNaN\nNaN\nU.S. Census Bureau\nNaN\nNaN\n006:004\n2019-10-22 15:36:29.0\n(census,)\n{'fn': 'American Community Survey Office', 'hasEmail': 'mailto:Acso.users.support@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2018/acs/acs5/subject', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe American Community Survey (ACS) is an ongoing survey that provides data every year -- giving communities the current information they need to plan investments and services. The ACS covers a broad range of topics about social, economic, demographic, and housing characteristics of the U.S. population. The subject tables include the following geographies: nation, all states (including DC and Puerto Rico), all metropolitan areas, all congressional districts, all counties, all places and all tracts. Subject tables provide an overview of the estimates available in a particular topic. The data are presented as both counts and percentages. There are over 66,000 variables in this dataset.\n006:07\npublic\nACS 5-Year Subject Tables\nTrue\nTrue\nTrue\n(acs, acs5, subject)\n2018.0\n\n\nACSST5Y2019\nNaN\nNaN\nU.S. Census Bureau\nNaN\nNaN\n006:004\n2020-04-03 00:00:00.0\n(census,)\n{'fn': 'American Community Survey Office', 'hasEmail': 'mailto:acso.users.support@census.gov'}\n{'@type': 'dcat:Distribution', 'accessURL': 'http://api.census.gov/data/2019/acs/acs5/subject', 'description': 'API endpoint', 'format': 'API', 'mediaType': 'application/json', 'title': 'API endpoint'}\nThe American Community Survey (ACS) is an ongoing survey that provides data every year -- giving communities the current information they need to plan investments and services. The ACS covers a broad range of topics about social, economic, demographic, and housing characteristics of the U.S. population. The subject tables include the following geographies: nation, all states (including DC and Puerto Rico), all metropolitan areas, all congressional districts, all counties, all places and all tracts. Subject tables provide an overview of the estimates available in a particular topic. The data are presented as both counts and percentages. There are over 66,000 variables in this dataset.\n006:07\npublic\nAmerican Community Survey: 5-Year Estimates: Subject Tables 5-Year\nTrue\nTrue\nTrue\n(acs, acs5, subject)\n2019.0\n\n\n\n\n244 rows × 19 columns\n\n\n\n\n\n\nStep 2: Initialize the API connection\n\nacs = cenpy.remote.APIConnection(\"ACSST5Y2016\")\nacs.variables.head(n=10)\n\n\n\n\n\n\n\n\n\nlabel\nconcept\npredicateType\ngroup\nlimit\npredicateOnly\nhasGeoCollectionSupport\nattributes\nrequired\nvalues\n\n\n\n\nfor\nCensus API FIPS 'for' clause\nCensus API Geography Specification\nfips-for\nN/A\n0\nTrue\nNaN\nNaN\nNaN\nNaN\n\n\nin\nCensus API FIPS 'in' clause\nCensus API Geography Specification\nfips-in\nN/A\n0\nTrue\nNaN\nNaN\nNaN\nNaN\n\n\nucgid\nUniform Census Geography Identifier clause\nCensus API Geography Specification\nucgid\nN/A\n0\nTrue\nTrue\nNaN\nNaN\nNaN\n\n\nS0804_C04_068E\nPublic transportation (excluding taxicab)!!Estimate!!Workers 16 years and over who did not work at home!!TIME ARRIVING AT WORK FROM HOME!!5:00 a.m. to 5:29 a.m.\nMEANS OF TRANSPORTATION TO WORK BY SELECTED CHARACTERISTICS FOR WORKPLACE GEOGRAPHY\nfloat\nS0804\n0\nNaN\nNaN\nS0804_C04_068EA,S0804_C04_068M,S0804_C04_068MA\nNaN\nNaN\n\n\nS0503_C02_078E\nForeign born; Born in Europe!!Estimate!!INDUSTRY!!Retail trade\nSELECTED CHARACTERISTICS OF THE FOREIGN-BORN POPULATION BY REGION OF BIRTH: EUROPE\nfloat\nS0503\n0\nNaN\nNaN\nS0503_C02_078EA,S0503_C02_078M,S0503_C02_078MA\nNaN\nNaN\n\n\nS0701PR_C01_028E\nTotal!!Estimate!!MARITAL STATUS!!Population 15 years and over\nGEOGRAPHIC MOBILITY BY SELECTED CHARACTERISTICS IN PUERTO RICO\nint\nS0701PR\n0\nNaN\nNaN\nS0701PR_C01_028EA,S0701PR_C01_028M,S0701PR_C01_028MA\nNaN\nNaN\n\n\nS0804_C04_067E\nPublic transportation (excluding taxicab)!!Estimate!!Workers 16 years and over who did not work at home!!TIME ARRIVING AT WORK FROM HOME!!12:00 a.m. to 4:59 a.m.\nMEANS OF TRANSPORTATION TO WORK BY SELECTED CHARACTERISTICS FOR WORKPLACE GEOGRAPHY\nfloat\nS0804\n0\nNaN\nNaN\nS0804_C04_067EA,S0804_C04_067M,S0804_C04_067MA\nNaN\nNaN\n\n\nS0503_C02_077E\nForeign born; Born in Europe!!Estimate!!INDUSTRY!!Wholesale trade\nSELECTED CHARACTERISTICS OF THE FOREIGN-BORN POPULATION BY REGION OF BIRTH: EUROPE\nfloat\nS0503\n0\nNaN\nNaN\nS0503_C02_077EA,S0503_C02_077M,S0503_C02_077MA\nNaN\nNaN\n\n\nS0701PR_C01_029E\nTotal!!Estimate!!MARITAL STATUS!!Population 15 years and over!!Never married\nGEOGRAPHIC MOBILITY BY SELECTED CHARACTERISTICS IN PUERTO RICO\nint\nS0701PR\n0\nNaN\nNaN\nS0701PR_C01_029EA,S0701PR_C01_029M,S0701PR_C01_029MA\nNaN\nNaN\n\n\nS0503_C02_076E\nForeign born; Born in Europe!!Estimate!!INDUSTRY!!Manufacturing\nSELECTED CHARACTERISTICS OF THE FOREIGN-BORN POPULATION BY REGION OF BIRTH: EUROPE\nfloat\nS0503\n0\nNaN\nNaN\nS0503_C02_076EA,S0503_C02_076M,S0503_C02_076MA\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\nStep 3: Find the variables we want to load\n\n# Assuming 'acs' is your ACSST5Y2016 APIConnection object\nvariables = [\"S1701_C01_001E\", \"S1701_C01_001M\", \"S1701_C02_001E\", \"S1701_C02_001M\", \"S1701_C03_001E\", \"S1701_C03_001M\"]\n\n# Florida's FIPS code is 12\nflorida_tracts = acs.query(\n    cols=variables, \n    geo_unit='tract:*', \n    geo_filter={'state':'12'}\n)\n\n# Display the first few rows of the retrieved data\nflorida_tracts.head()\n\n\n\n\n\n\n\n\n\nS1701_C01_001E\nS1701_C01_001M\nS1701_C02_001E\nS1701_C02_001M\nS1701_C03_001E\nS1701_C03_001M\nstate\ncounty\ntract\n\n\n\n\n0\n2504\n240\n224\n107\n8.9\n4.4\n12\n071\n001701\n\n\n1\n6075\n590\n227\n210\n3.7\n3.4\n12\n071\n010303\n\n\n2\n7342\n1045\n1603\n772\n21.8\n9.4\n12\n071\n010307\n\n\n3\n9873\n788\n1548\n617\n15.7\n6.1\n12\n071\n010103\n\n\n4\n4549\n727\n757\n426\n16.6\n8.7\n12\n071\n001801\n\n\n\n\n\n\n\n\n\nflorida_tracts['GEOID'] = florida_tracts['state'] + florida_tracts['county'] + florida_tracts['tract']\n# florida_tracts = florida_tracts.set_index('GEOID')\nflorida_tracts.head()\n\n\n\n\n\n\n\n\n\nS1701_C01_001E\nS1701_C01_001M\nS1701_C02_001E\nS1701_C02_001M\nS1701_C03_001E\nS1701_C03_001M\nstate\ncounty\ntract\nGEOID\n\n\n\n\n0\n2504\n240\n224\n107\n8.9\n4.4\n12\n071\n001701\n12071001701\n\n\n1\n6075\n590\n227\n210\n3.7\n3.4\n12\n071\n010303\n12071010303\n\n\n2\n7342\n1045\n1603\n772\n21.8\n9.4\n12\n071\n010307\n12071010307\n\n\n3\n9873\n788\n1548\n617\n15.7\n6.1\n12\n071\n010103\n12071010103\n\n\n4\n4549\n727\n757\n426\n16.6\n8.7\n12\n071\n001801\n12071001801\n\n\n\n\n\n\n\n\n\nvars=['S1701_C01_001','S1701_C02_001','S1701_C03_001']\nfor i in vars:\n    florida_tracts[f\"{i}E\"] = florida_tracts[f\"{i}E\"].astype('float')\n    florida_tracts[f\"{i}M\"] = florida_tracts[f\"{i}M\"].astype('float')\n\nflorida_tracts.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4245 entries, 0 to 4244\nData columns (total 10 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   S1701_C01_001E  4245 non-null   float64\n 1   S1701_C01_001M  4245 non-null   float64\n 2   S1701_C02_001E  4245 non-null   float64\n 3   S1701_C02_001M  4245 non-null   float64\n 4   S1701_C03_001E  4245 non-null   float64\n 5   S1701_C03_001M  4245 non-null   float64\n 6   state           4245 non-null   object \n 7   county          4245 non-null   object \n 8   tract           4245 non-null   object \n 9   GEOID           4245 non-null   object \ndtypes: float64(6), object(4)\nmemory usage: 331.8+ KB\n\n\n\nflorida_tracts.sort_values(by='S1701_C01_001E',ascending=True).head(10)\n\n\n\n\n\n\n\n\n\nS1701_C01_001E\nS1701_C01_001M\nS1701_C02_001E\nS1701_C02_001M\nS1701_C03_001E\nS1701_C03_001M\nstate\ncounty\ntract\nGEOID\n\n\n\n\n4188\n0.0\n13.0\n0.0\n13.0\n-666666666.0\n-222222222.0\n12\n127\n990000\n12127990000\n\n\n1233\n0.0\n13.0\n0.0\n13.0\n-666666666.0\n-222222222.0\n12\n099\n980200\n12099980200\n\n\n1234\n0.0\n13.0\n0.0\n13.0\n-666666666.0\n-222222222.0\n12\n099\n980400\n12099980400\n\n\n1235\n0.0\n13.0\n0.0\n13.0\n-666666666.0\n-222222222.0\n12\n099\n980500\n12099980500\n\n\n2100\n0.0\n13.0\n0.0\n13.0\n-666666666.0\n-222222222.0\n12\n031\n990000\n12031990000\n\n\n2099\n0.0\n13.0\n0.0\n13.0\n-666666666.0\n-222222222.0\n12\n099\n980100\n12099980100\n\n\n1615\n0.0\n13.0\n0.0\n13.0\n-666666666.0\n-222222222.0\n12\n029\n990000\n12029990000\n\n\n1236\n0.0\n13.0\n0.0\n13.0\n-666666666.0\n-222222222.0\n12\n099\n990000\n12099990000\n\n\n1237\n0.0\n13.0\n0.0\n13.0\n-666666666.0\n-222222222.0\n12\n099\n990100\n12099990100\n\n\n2644\n0.0\n13.0\n0.0\n13.0\n-666666666.0\n-222222222.0\n12\n091\n990200\n12091990200\n\n\n\n\n\n\n\n\n\nvariables = [\"S1701_C01_001E\", \"S1701_C01_001M\", \"S1701_C02_001E\", \"S1701_C02_001M\", \"S1701_C03_001E\", \"S1701_C03_001M\"]\nfor i in variables:\n    florida_tracts[i] = florida_tracts[i].replace(0, np.nan)\n\nflorida_tracts.sort_values(by='S1701_C01_001E',ascending=True).head(10)\n\n\n\n\n\n\n\n\n\nS1701_C01_001E\nS1701_C01_001M\nS1701_C02_001E\nS1701_C02_001M\nS1701_C03_001E\nS1701_C03_001M\nstate\ncounty\ntract\nGEOID\n\n\n\n\n466\n4.0\n9.0\n4.0\n9.0\n100.0\n100.0\n12\n087\n980100\n12087980100\n\n\n193\n6.0\n9.0\n4.0\n6.0\n66.7\n66.7\n12\n073\n001300\n12073001300\n\n\n465\n7.0\n11.0\nNaN\n13.0\nNaN\n100.0\n12\n087\n980000\n12087980000\n\n\n2439\n7.0\n11.0\n7.0\n11.0\n100.0\n100.0\n12\n086\n008904\n12086008904\n\n\n3313\n12.0\n18.0\nNaN\n13.0\nNaN\n95.0\n12\n119\n980000\n12119980000\n\n\n1858\n16.0\n24.0\nNaN\n13.0\nNaN\n82.3\n12\n057\n980500\n12057980500\n\n\n2387\n23.0\n28.0\nNaN\n13.0\nNaN\n68.6\n12\n119\n911000\n12119911000\n\n\n2810\n25.0\n18.0\nNaN\n13.0\nNaN\n65.8\n12\n055\n980000\n12055980000\n\n\n3431\n32.0\n52.0\n32.0\n52.0\n100.0\n58.2\n12\n105\n980000\n12105980000\n\n\n3388\n38.0\n18.0\n6.0\n6.0\n15.8\n18.9\n12\n001\n000902\n12001000902\n\n\n\n\n\n\n\n\n\ntype(florida_tracts)\n\npandas.core.frame.DataFrame\n\n\n\nflorida_tracts.isnull().sum()\n\nS1701_C01_001E    81\nS1701_C01_001M     0\nS1701_C02_001E    87\nS1701_C02_001M     0\nS1701_C03_001E     6\nS1701_C03_001M     0\nstate              0\ncounty             0\ntract              0\nGEOID              0\ndtype: int64",
    "crumbs": [
      "Work in Progress",
      "Mapping Uncertainty in ACS Data"
    ]
  },
  {
    "objectID": "Work in Progress/mapping_uncertainty.html#task-3-calculate-coefficients-of-variation-for-acs-data",
    "href": "Work in Progress/mapping_uncertainty.html#task-3-calculate-coefficients-of-variation-for-acs-data",
    "title": "Mapping Uncertainty in ACS Data",
    "section": "Task 3: Calculate Coefficients of Variation for ACS Data ",
    "text": "Task 3: Calculate Coefficients of Variation for ACS Data \nTo calculate the Standard Error (SE) and Coefficient of Variation (CV) for the variables of interest from the American Community Survey (ACS) data, we use the following formulas:\n\nThe Standard Error (SE) can be derived from the Margin of Error (MOE) provided for each estimate. The ACS provides MOE at a 90% confidence level, which is approximately 1.645 times the standard error. Therefore, the formula to calculate the SE from the MOE is:\n\n\\[\nSE = \\frac{MOE}{1.645}\n\\]\n\nThe Coefficient of Variation (CV) is a measure of relative variability and is calculated as the ratio of the standard error (SE) to the estimate itself, expressed as a percentage. The formula for CV is:\n\n\\[\nCV = \\left( \\frac{SE}{Estimate} \\right) \\times 100\n\\]\nFor each variable of interest, we apply these formulas. Let’s take the variable S1701_C01_001E (Estimate) and its corresponding margin of error S1701_C01_001M (MOE) as an example:\n\nFirst, calculate the Standard Error (SE) for S1701_C01_001E:\n\nflorida_tracts['S1701_C01_001E_SE'] = florida_tracts['S1701_C01_001M'] / 1.645\n\nThen, calculate the Coefficient of Variation (CV) for S1701_C01_001E:\n\nflorida_tracts['S1701_C01_001E_CV'] = (florida_tracts['S1701_C01_001E_SE'] / florida_tracts['S1701_C01_001E']) * 100\n\nRepeat these steps for all the variables of interest.\n\n\nflorida_tracts.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4245 entries, 0 to 4244\nData columns (total 10 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   S1701_C01_001E  4164 non-null   float64\n 1   S1701_C01_001M  4245 non-null   float64\n 2   S1701_C02_001E  4158 non-null   float64\n 3   S1701_C02_001M  4245 non-null   float64\n 4   S1701_C03_001E  4239 non-null   float64\n 5   S1701_C03_001M  4245 non-null   float64\n 6   state           4245 non-null   object \n 7   county          4245 non-null   object \n 8   tract           4245 non-null   object \n 9   GEOID           4245 non-null   object \ndtypes: float64(6), object(4)\nmemory usage: 331.8+ KB\n\n\n\nvars=['S1701_C01_001','S1701_C02_001','S1701_C03_001']\nfor i in vars:\n    florida_tracts[f\"{i}_SE\"] = florida_tracts[f\"{i}M\"] / 1.645\n    florida_tracts[f\"{i}_CV\"] = florida_tracts[f\"{i}_SE\"] / florida_tracts[f\"{i}E\"] * 100\n\nflorida_tracts.head()\n\n\n\n\n\n\n\n\n\nS1701_C01_001E\nS1701_C01_001M\nS1701_C02_001E\nS1701_C02_001M\nS1701_C03_001E\nS1701_C03_001M\nstate\ncounty\ntract\nGEOID\nS1701_C01_001_SE\nS1701_C01_001_CV\nS1701_C02_001_SE\nS1701_C02_001_CV\nS1701_C03_001_SE\nS1701_C03_001_CV\n\n\n\n\n0\n2504.0\n240.0\n224.0\n107.0\n8.9\n4.4\n12\n071\n001701\n12071001701\n145.896657\n5.826544\n65.045593\n29.038211\n2.674772\n30.053618\n\n\n1\n6075.0\n590.0\n227.0\n210.0\n3.7\n3.4\n12\n071\n010303\n12071010303\n358.662614\n5.903911\n127.659574\n56.237698\n2.066869\n55.861332\n\n\n2\n7342.0\n1045.0\n1603.0\n772.0\n21.8\n9.4\n12\n071\n010307\n12071010307\n635.258359\n8.652388\n469.300912\n29.276414\n5.714286\n26.212320\n\n\n3\n9873.0\n788.0\n1548.0\n617.0\n15.7\n6.1\n12\n071\n010103\n12071010103\n479.027356\n4.851893\n375.075988\n24.229715\n3.708207\n23.619151\n\n\n4\n4549.0\n727.0\n757.0\n426.0\n16.6\n8.7\n12\n071\n001801\n12071001801\n441.945289\n9.715218\n258.966565\n34.209586\n5.288754\n31.859963\n\n\n\n\n\n\n\n\n\nflorida_tracts.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4245 entries, 0 to 4244\nData columns (total 16 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   S1701_C01_001E    4245 non-null   float64\n 1   S1701_C01_001M    4245 non-null   float64\n 2   S1701_C02_001E    4245 non-null   float64\n 3   S1701_C02_001M    4245 non-null   float64\n 4   S1701_C03_001E    4245 non-null   float64\n 5   S1701_C03_001M    4245 non-null   float64\n 6   state             4245 non-null   object \n 7   county            4245 non-null   object \n 8   tract             4245 non-null   object \n 9   GEOID             4245 non-null   object \n 10  S1701_C01_001_SE  4245 non-null   float64\n 11  S1701_C01_001_CV  4245 non-null   float64\n 12  S1701_C02_001_SE  4245 non-null   float64\n 13  S1701_C02_001_CV  4245 non-null   float64\n 14  S1701_C03_001_SE  4245 non-null   float64\n 15  S1701_C03_001_CV  4245 non-null   float64\ndtypes: float64(12), object(4)\nmemory usage: 530.8+ KB\n\n\n\nflorida_tracts.isnull().sum()\n\nS1701_C01_001E      81\nS1701_C01_001M       0\nS1701_C02_001E      87\nS1701_C02_001M       0\nS1701_C03_001E       6\nS1701_C03_001M       0\nstate                0\ncounty               0\ntract                0\nGEOID                0\nS1701_C01_001_SE     0\nS1701_C01_001_CV    81\nS1701_C02_001_SE     0\nS1701_C02_001_CV    87\nS1701_C03_001_SE     0\nS1701_C03_001_CV     6\ndtype: int64\n\n\n\nflorida_tracts.hist(bins=100, figsize=(10,10))\n\narray([[&lt;Axes: title={'center': 'S1701_C01_001E'}&gt;,\n        &lt;Axes: title={'center': 'S1701_C01_001M'}&gt;,\n        &lt;Axes: title={'center': 'S1701_C02_001E'}&gt;],\n       [&lt;Axes: title={'center': 'S1701_C02_001M'}&gt;,\n        &lt;Axes: title={'center': 'S1701_C03_001E'}&gt;,\n        &lt;Axes: title={'center': 'S1701_C03_001M'}&gt;],\n       [&lt;Axes: title={'center': 'S1701_C01_001_SE'}&gt;,\n        &lt;Axes: title={'center': 'S1701_C01_001_CV'}&gt;,\n        &lt;Axes: title={'center': 'S1701_C02_001_SE'}&gt;],\n       [&lt;Axes: title={'center': 'S1701_C02_001_CV'}&gt;,\n        &lt;Axes: title={'center': 'S1701_C03_001_SE'}&gt;,\n        &lt;Axes: title={'center': 'S1701_C03_001_CV'}&gt;]], dtype=object)",
    "crumbs": [
      "Work in Progress",
      "Mapping Uncertainty in ACS Data"
    ]
  },
  {
    "objectID": "Work in Progress/mapping_uncertainty.html#task-4-categorize-census-tracts-according-to-data-quality-as-indicated-by-cvs",
    "href": "Work in Progress/mapping_uncertainty.html#task-4-categorize-census-tracts-according-to-data-quality-as-indicated-by-cvs",
    "title": "Mapping Uncertainty in ACS Data",
    "section": "Task 4: Categorize Census Tracts According to Data Quality as Indicated by CVs ",
    "text": "Task 4: Categorize Census Tracts According to Data Quality as Indicated by CVs \n\nfor i in vars:\n    conditions = [\n        (florida_tracts[f\"{i}_CV\"] &lt; 0),\n        (florida_tracts[f\"{i}_CV\"] &lt; 12),\n        (florida_tracts[f\"{i}_CV\"] &lt; 40),\n        (florida_tracts[f\"{i}_CV\"] &gt;= 40)\n    ]\n    \n    # Define choices corresponding to each condition\n    choices = [\"No CV\", \"High\", \"Medium\", \"Low\"]\n    \n    # Use np.select to apply conditions and choices to the dataframe\n    florida_tracts[f\"{i}_rating\"] = np.select(conditions, choices, default=np.nan)\n\nflorida_tracts.head()\n\n\n\n\n\n\n\n\n\nS1701_C01_001E\nS1701_C01_001M\nS1701_C02_001E\nS1701_C02_001M\nS1701_C03_001E\nS1701_C03_001M\nstate\ncounty\ntract\nGEOID\nS1701_C01_001_SE\nS1701_C01_001_CV\nS1701_C02_001_SE\nS1701_C02_001_CV\nS1701_C03_001_SE\nS1701_C03_001_CV\nS1701_C01_001_rating\nS1701_C02_001_rating\nS1701_C03_001_rating\n\n\n\n\n0\n2504.0\n240.0\n224.0\n107.0\n8.9\n4.4\n12\n071\n001701\n12071001701\n145.896657\n5.826544\n65.045593\n29.038211\n2.674772\n30.053618\nHigh\nMedium\nMedium\n\n\n1\n6075.0\n590.0\n227.0\n210.0\n3.7\n3.4\n12\n071\n010303\n12071010303\n358.662614\n5.903911\n127.659574\n56.237698\n2.066869\n55.861332\nHigh\nLow\nLow\n\n\n2\n7342.0\n1045.0\n1603.0\n772.0\n21.8\n9.4\n12\n071\n010307\n12071010307\n635.258359\n8.652388\n469.300912\n29.276414\n5.714286\n26.212320\nHigh\nMedium\nMedium\n\n\n3\n9873.0\n788.0\n1548.0\n617.0\n15.7\n6.1\n12\n071\n010103\n12071010103\n479.027356\n4.851893\n375.075988\n24.229715\n3.708207\n23.619151\nHigh\nMedium\nMedium\n\n\n4\n4549.0\n727.0\n757.0\n426.0\n16.6\n8.7\n12\n071\n001801\n12071001801\n441.945289\n9.715218\n258.966565\n34.209586\n5.288754\n31.859963\nHigh\nMedium\nMedium\n\n\n\n\n\n\n\n\n\nflorida_tracts.to_csv('florida_tracts.csv')",
    "crumbs": [
      "Work in Progress",
      "Mapping Uncertainty in ACS Data"
    ]
  },
  {
    "objectID": "Work in Progress/mapping_uncertainty.html#task-5-join-the-file-to-the-florida-tracts-geometry",
    "href": "Work in Progress/mapping_uncertainty.html#task-5-join-the-file-to-the-florida-tracts-geometry",
    "title": "Mapping Uncertainty in ACS Data",
    "section": "Task 5: Join the File to the Florida Tracts Geometry ",
    "text": "Task 5: Join the File to the Florida Tracts Geometry \n\nacs.geographies['fips']\n\n\n\n\n\n\n\n\n\nname\ngeoLevelDisplay\nreferenceDate\nrequires\nwildcard\noptionalWithWCFor\n\n\n\n\n0\nus\n010\n2016-01-01\nNaN\nNaN\nNaN\n\n\n1\nregion\n020\n2016-01-01\nNaN\nNaN\nNaN\n\n\n2\ndivision\n030\n2016-01-01\nNaN\nNaN\nNaN\n\n\n3\nstate\n040\n2016-01-01\nNaN\nNaN\nNaN\n\n\n4\ncounty\n050\n2016-01-01\n[state]\n[state]\nstate\n\n\n5\ncounty subdivision\n060\n2016-01-01\n[state, county]\n[county]\ncounty\n\n\n6\nsubminor civil division\n067\n2016-01-01\n[state, county, county subdivision]\nNaN\nNaN\n\n\n7\ntract\n140\n2016-01-01\n[state, county]\n[county]\ncounty\n\n\n8\nplace\n160\n2016-01-01\n[state]\n[state]\nstate\n\n\n9\nconsolidated city\n170\n2016-01-01\n[state]\n[state]\nstate\n\n\n10\nalaska native regional corporation\n230\n2016-01-01\n[state]\n[state]\nstate\n\n\n11\namerican indian area/alaska native area/hawaiian home land\n250\n2016-01-01\nNaN\nNaN\nNaN\n\n\n12\namerican indian tribal subdivision\n251\n2016-01-01\n[american indian area/alaska native area/hawaiian home land]\n[american indian area/alaska native area/hawaiian home land]\namerican indian area/alaska native area/hawaiian home land\n\n\n13\namerican indian area/alaska native area (reservation or statistical entity only)\n252\n2016-01-01\n[american indian area/alaska native area/hawaiian home land]\n[american indian area/alaska native area/hawaiian home land]\namerican indian area/alaska native area/hawaiian home land\n\n\n14\namerican indian area (off-reservation trust land only)/hawaiian home land\n254\n2016-01-01\n[american indian area/alaska native area/hawaiian home land]\n[american indian area/alaska native area/hawaiian home land]\namerican indian area/alaska native area/hawaiian home land\n\n\n15\ntribal census tract\n256\n2016-01-01\n[american indian area/alaska native area/hawaiian home land]\nNaN\nNaN\n\n\n16\nstate (or part)\n260\n2016-01-01\n[american indian area/alaska native area/hawaiian home land]\nNaN\nNaN\n\n\n17\nmetropolitan statistical area/micropolitan statistical area\n310\n2016-01-01\nNaN\nNaN\nNaN\n\n\n18\nprincipal city (or part)\n312\n2016-01-01\n[metropolitan statistical area/micropolitan statistical area, state (or part)]\nNaN\nNaN\n\n\n19\nmetropolitan division\n314\n2016-01-01\n[metropolitan statistical area/micropolitan statistical area]\nNaN\nNaN\n\n\n20\ncombined statistical area\n330\n2016-01-01\nNaN\nNaN\nNaN\n\n\n21\ncombined new england city and town area\n335\n2016-01-01\nNaN\nNaN\nNaN\n\n\n22\nnew england city and town area\n350\n2016-01-01\nNaN\nNaN\nNaN\n\n\n23\nprincipal city\n352\n2016-01-01\n[new england city and town area, state (or part)]\n[state (or part)]\nstate (or part)\n\n\n24\nnecta division\n355\n2016-01-01\n[new england city and town area]\nNaN\nNaN\n\n\n25\nurban area\n400\n2016-01-01\nNaN\nNaN\nNaN\n\n\n26\ncongressional district\n500\n2016-01-01\n[state]\n[state]\nstate\n\n\n27\nstate legislative district (upper chamber)\n610\n2016-01-01\n[state]\nNaN\nNaN\n\n\n28\nstate legislative district (lower chamber)\n620\n2016-01-01\n[state]\nNaN\nNaN\n\n\n29\npublic use microdata area\n795\n2016-01-01\n[state]\n[state]\nstate\n\n\n30\nzip code tabulation area\n860\n2016-01-01\n[state]\n[state]\nstate\n\n\n31\nschool district (elementary)\n950\n2016-01-01\n[state]\nNaN\nNaN\n\n\n32\nschool district (secondary)\n960\n2016-01-01\n[state]\nNaN\nNaN\n\n\n33\nschool district (unified)\n970\n2016-01-01\n[state]\nNaN\nNaN\n\n\n\n\n\n\n\n\n\ncounties = cenpy.explorer.fips_table(\"COUNTY\")\ncounties.head()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\nAL\n1\n1\nAutauga County\nH1\n\n\n1\nAL\n1\n3\nBaldwin County\nH1\n\n\n2\nAL\n1\n5\nBarbour County\nH1\n\n\n3\nAL\n1\n7\nBibb County\nH1\n\n\n4\nAL\n1\n9\nBlount County\nH1\n\n\n\n\n\n\n\n\n\nimport pygris\n\n\npygris.tracts?\n\nSignature:\npygris.tracts(\n    state=None,\n    county=None,\n    cb=False,\n    year=None,\n    cache=False,\n    subset_by=None,\n)\nDocstring:\n Load a Census tracts shapefile into Python as a GeoDataFrame\n\nParameters\n----------\nstate : str \n    The state name, state abbreviation, or two-digit FIPS code of the desired state. \n    If None, Census tracts for the entire United States will be downloaded if available for that \n    year / dataset combination.  \ncounty : str\n    The county name or three-digit FIPS code of the desired county. If None, Census tracts\n    for the selected state will be downloaded. \ncb : bool \n    If set to True, download a generalized (1:500k) cartographic boundary file.  \n    Defaults to False (the regular TIGER/Line file).\nyear : int \n    The year of the TIGER/Line or cartographic boundary shapefile. \ncache : bool \n    If True, the function will download a Census shapefile to a cache directory \n    on the user's computer for future access.  If False, the function will load\n    the shapefile directly from the Census website.      \nsubset_by : tuple, int, slice, dict, geopandas.GeoDataFrame, or geopandas.GeoSeries\n    An optional directive telling pygris to return a subset of data using \n    underlying arguments in geopandas.read_file().  \n    subset_by operates as follows:\n        * If a user supplies a tuple of format (minx, miny, maxx, maxy), \n        it will be interpreted as a bounding box and rows will be returned\n        that intersect that bounding box;\n        * If a user supplies a integer or a slice object, the first n rows\n        (or the rows defined by the slice object) will be returned;\n        * If a user supplies an object of type geopandas.GeoDataFrame\n        or of type geopandas.GeoSeries, rows that intersect the input \n        object will be returned. CRS misalignment will be resolved \n        internally.  \n        * A dict of format {\"address\": \"buffer_distance\"} will return rows\n        that intersect a buffer of a given distance (in meters) around an \n        input address.  \n\nReturns\n----------\ngeopandas.GeoDataFrame: A GeoDataFrame of Census tracts.\n\n\nNotes\n----------\nSee https://www2.census.gov/geo/pdfs/reference/GARM/Ch10GARM.pdf for more information.    \nFile:      ~/anaconda3/envs/eda/lib/python3.12/site-packages/pygris/enumeration_units.py\nType:      function\n\n\n\nfl_tracts_geo=pygris.tracts(\n    state=\"FL\",\n    year=2016,\n)\n\nfl_tracts_geo.plot()\n\nUsing FIPS code '12' for input 'FL'\n\n\n\n\n\n\n\n\n\n\nlen(fl_tracts_geo)\n\n4245\n\n\n\nfl_tracts_geo.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 4245 entries, 0 to 4244\nData columns (total 13 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   STATEFP   4245 non-null   object  \n 1   COUNTYFP  4245 non-null   object  \n 2   TRACTCE   4245 non-null   object  \n 3   GEOID     4245 non-null   object  \n 4   NAME      4245 non-null   object  \n 5   NAMELSAD  4245 non-null   object  \n 6   MTFCC     4245 non-null   object  \n 7   FUNCSTAT  4245 non-null   object  \n 8   ALAND     4245 non-null   int64   \n 9   AWATER    4245 non-null   int64   \n 10  INTPTLAT  4245 non-null   object  \n 11  INTPTLON  4245 non-null   object  \n 12  geometry  4245 non-null   geometry\ndtypes: geometry(1), int64(2), object(10)\nmemory usage: 431.3+ KB",
    "crumbs": [
      "Work in Progress",
      "Mapping Uncertainty in ACS Data"
    ]
  },
  {
    "objectID": "Work in Progress/mapping_uncertainty.html#task-6-merge-the-demographic-data-with-geometries",
    "href": "Work in Progress/mapping_uncertainty.html#task-6-merge-the-demographic-data-with-geometries",
    "title": "Mapping Uncertainty in ACS Data",
    "section": "Task 6: Merge the demographic data with geometries ",
    "text": "Task 6: Merge the demographic data with geometries \n\nflorida_demo_final=fl_tracts_geo.merge(\n    florida_tracts, \n    on='GEOID')\n\n\nflorida_demo_final.plot(column='S1701_C01_001E', legend=True, figsize=(10,10))",
    "crumbs": [
      "Work in Progress",
      "Mapping Uncertainty in ACS Data"
    ]
  },
  {
    "objectID": "Work in Progress/mapping_uncertainty.html#task-7-map-the-poverty-rate-data-and-include-data-quality-as-indicated-by-cvs",
    "href": "Work in Progress/mapping_uncertainty.html#task-7-map-the-poverty-rate-data-and-include-data-quality-as-indicated-by-cvs",
    "title": "Mapping Uncertainty in ACS Data",
    "section": "Task 7: Map the Poverty Rate Data and Include Data Quality as Indicated by CVs ",
    "text": "Task 7: Map the Poverty Rate Data and Include Data Quality as Indicated by CVs \nFor this visualization, we’ll focus on mapping the poverty rate using a graduated color scheme based on S1701_C01_001E and overlaying this with hatched patterns based on S1701_C01_001_rating to indicate data quality.\nWe’ll adjust transparency as needed to make both layers visible and interpretable.\n\nflorida_demo_final.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 4245 entries, 0 to 4244\nData columns (total 31 columns):\n #   Column                Non-Null Count  Dtype   \n---  ------                --------------  -----   \n 0   STATEFP               4245 non-null   object  \n 1   COUNTYFP              4245 non-null   object  \n 2   TRACTCE               4245 non-null   object  \n 3   GEOID                 4245 non-null   object  \n 4   NAME                  4245 non-null   object  \n 5   NAMELSAD              4245 non-null   object  \n 6   MTFCC                 4245 non-null   object  \n 7   FUNCSTAT              4245 non-null   object  \n 8   ALAND                 4245 non-null   int64   \n 9   AWATER                4245 non-null   int64   \n 10  INTPTLAT              4245 non-null   object  \n 11  INTPTLON              4245 non-null   object  \n 12  geometry              4245 non-null   geometry\n 13  S1701_C01_001E        4164 non-null   float64 \n 14  S1701_C01_001M        4245 non-null   float64 \n 15  S1701_C02_001E        4158 non-null   float64 \n 16  S1701_C02_001M        4245 non-null   float64 \n 17  S1701_C03_001E        4239 non-null   float64 \n 18  S1701_C03_001M        4245 non-null   float64 \n 19  state                 4245 non-null   object  \n 20  county                4245 non-null   object  \n 21  tract                 4245 non-null   object  \n 22  S1701_C01_001_SE      4245 non-null   float64 \n 23  S1701_C01_001_CV      4164 non-null   float64 \n 24  S1701_C02_001_SE      4245 non-null   float64 \n 25  S1701_C02_001_CV      4158 non-null   float64 \n 26  S1701_C03_001_SE      4245 non-null   float64 \n 27  S1701_C03_001_CV      4239 non-null   float64 \n 28  S1701_C01_001_rating  4245 non-null   object  \n 29  S1701_C02_001_rating  4245 non-null   object  \n 30  S1701_C03_001_rating  4245 non-null   object  \ndtypes: float64(12), geometry(1), int64(2), object(16)\nmemory usage: 1.0+ MB\n\n\n\nS1701_C01_001E: poverty ref population estimate\nS1701_C02_001E: poverty population estimate\nS1701_C03_001E: percentage of poverty population estimate\n\n\n# florida_demo_final.explore(column=\"S1701_C01_001E\", tiles=\"CartoDB positron\")\n\n\nStep 1: Visualize Poverty Rates\nFirst, we’ll plot the poverty rates with a graduated color scheme. Since we want a red color scheme, we’ll use a colormap ranging from light to dark red to represent increasing poverty rates. We’ll make this layer less transparent to keep it visible under the data quality overlay.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\n\n# Step 1: Plot poverty rates\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\nflorida_demo_final.plot(column='S1701_C01_001E', ax=ax, legend=True,\n                        legend_kwds={'label': \"Poverty Rate (%)\", 'orientation': \"horizontal\"},\n                        cmap='Reds', scheme='NaturalBreaks', k=6, edgecolor='none', alpha=0.8)\n\n# Adjust as necessary for your dataset and preferences\nplt.title('Florida Poverty Rates & Data Quality')\nplt.axis('off')\n\n# Placeholder for Step 2: This will involve plotting the data quality overlay\n# This step will be added after explaining the logic for data quality visualization\n\nplt.show()\n\nTypeError: Legend.__init__() got an unexpected keyword argument 'label'\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Overlay Data Quality Information\nNext, we’ll add the data quality overlay. For this, we’ll categorize the tracts based on their S1701_C01_001_rating and use hatched fills to indicate the quality. Different hatch patterns (or colors) will be used for “High”, “Medium”, “Low”, and “No CV” categories. This layer will be more transparent so the poverty rates underneath are still visible.\n\nflorida_demo_final['S1701_C01_001_rating'] = florida_demo_final['S1701_C01_001_rating'].astype('category')\nflorida_demo_final['S1701_C01_001_rating'].cat.set_categories(['High', 'Medium', 'Low', 'No CV'], ordered=True)\n\n# Plotting\nfig, ax = plt.subplots(figsize=(10, 10))\nflorida_demo_final.plot(column='S1701_C01_001E', ax=ax, legend=True,\n                        # legend_kwds={'label': \"Poverty Rate (%)\", 'orientation': \"horizontal\"},\n                        cmap='Reds', scheme='NaturalBreaks', k=6, edgecolor='none')\n\n# Overlap with color intensity for uncertainty\nflorida_demo_final.plot(column='S1701_C01_001_rating', ax=ax, legend=True,\n                        cmap='Greys', alpha=0.3, edgecolor='none')  # Using greyscale for simplicity\n\nplt.axis('off')\nplt.show()",
    "crumbs": [
      "Work in Progress",
      "Mapping Uncertainty in ACS Data"
    ]
  },
  {
    "objectID": "Topical/sovi_svi.html",
    "href": "Topical/sovi_svi.html",
    "title": "Comparing Indices: CDC’s SVI and HVRI’s SoVI",
    "section": "",
    "text": "Integrity Statement: I used github GPT copilot to validate the code for this notebook. I also used the code from the following sources:\n\nSovi HVRI: https://sc.edu/study/colleges_schools/artsandsciences/centers_and_institutes/hvri/data_and_resources/sovi/sovi_data/index.php\nCDC SVI: https://www.atsdr.cdc.gov/placeandhealth/svi/data_documentation_download.html\nCenpy Geographic Data: https://cenpy-devs.github.io/cenpy/\n\n\n\n\nimport os\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nsvi = pd.read_excel('Assignment/SVI2016_US_COUNTY_Assignment.xlsx')\nsvi\n\n\n\n\n\n\n\n\n\nFID\nST\nSTATE\nST_ABBR\nCOUNTY\nGEOID\nLOCATION\nAREA_SQMI\nE_TOTPOP\nM_TOTPOP\nE_HU\nM_HU\nE_HH\nM_HH\nE_POV\nM_POV\nE_UNEMP\nM_UNEMP\nE_PCI\nM_PCI\nE_NOHSDP\nM_NOHSDP\nE_AGE65\nM_AGE65\nE_AGE17\nM_AGE17\nE_DISABL\nM_DISABL\nE_SNGPNT\nM_SNGPNT\nE_MINRTY\nM_MINRTY\nE_LIMENG\nM_LIMENG\nE_MUNIT\nM_MUNIT\nE_MOBILE\nM_MOBILE\nE_CROWD\nM_CROWD\nE_NOVEH\nM_NOVEH\nE_GROUPQ\nM_GROUPQ\nEP_POV\nMP_POV\nEP_UNEMP\nMP_UNEMP\nEP_PCI\nMP_PCI\nEP_NOHSDP\nMP_NOHSDP\nEP_AGE65\nMP_AGE65\nEP_AGE17\nMP_AGE17\nEP_DISABL\nMP_DISABL\nEP_SNGPNT\nMP_SNGPNT\nEP_MINRTY\nMP_MINRTY\nEP_LIMENG\nMP_LIMENG\nEP_MUNIT\nMP_MUNIT\nEP_MOBILE\nMP_MOBILE\nEP_CROWD\nMP_CROWD\nEP_NOVEH\nMP_NOVEH\nEP_GROUPQ\nMP_GROUPQ\nEPL_POV\nEPL_UNEMP\nEPL_PCI\nEPL_NOHSDP\nSPL_THEME1\nRPL_THEME1\nEPL_AGE65\nEPL_AGE17\nEPL_DISABL\nEPL_SNGPNT\nSPL_THEME2\nRPL_THEME2\nEPL_MINRTY\nEPL_LIMENG\nSPL_THEME3\nRPL_THEME3\nEPL_MUNIT\nEPL_MOBILE\nEPL_CROWD\nEPL_NOVEH\nEPL_GROUPQ\nSPL_THEME4\nRPL_THEME4\nSPL_THEMES\nRPL_THEMES\nF_POV\nF_UNEMP\nF_PCI\nF_NOHSDP\nF_THEME1\nF_AGE65\nF_AGE17\nF_DISABL\nF_SNGPNT\nF_THEME2\nF_MINRTY\nF_LIMENG\nF_THEME3\nF_MUNIT\nF_MOBILE\nF_CROWD\nF_NOVEH\nF_GROUPQ\nF_THEME4\nF_TOTAL\nE_UNINSUR\nM_UNINSUR\nEP_UNINSUR\nMP_UNINSUR\nE_DAYPOP\n\n\n\n\n0\n0\n1\nALABAMA\nAL\nAutauga\n1001\nAutauga County, Alabama\n594.446120\n55049\n0\n22714\n75\n20800\n391\n6697\n1037\n1437\n277\n26168\n1221\n4528\n445\n7695\n104\n13853\n34\n10009\n850\n1516\n267.0\n13386\n161.0\n432\n163.3\n1034\n329.9\n4095\n379\n254\n104.5\n1024\n242\n490\n163\n12.3\n1.9\n5.6\n1.1\n26168\n1221\n12.4\n1.2\n14.0\n0.2\n25.2\n0.1\n18.4\n1.6\n7.3\n1.3\n24.3\n0.3\n0.8\n0.3\n4.6\n1.5\n18.0\n1.7\n1.2\n0.5\n4.9\n1.1\n0.9\n0.3\n0.2824\n0.3298\n0.3607\n0.4744\n1.4473\nNaN\n0.1964\n0.8313\n0.7380\n0.3200\n2.0856\nNaN\n0.6339\n0.5355\n1.1694\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4852\n649\n8.9\n1.2\n40854\n\n\n1\n1341\n1\nALABAMA\nAL\nBaldwin\n1003\nBaldwin County, Alabama\n1589.807425\n199510\n0\n107579\n202\n75149\n1285\n25551\n1920\n5887\n674\n28069\n733\n13956\n974\n37338\n79\n44270\n0\n27390\n1430\n4494\n589.8\n33560\n245.0\n1540\n489.6\n19711\n936.9\n12829\n935\n958\n219.1\n2303\n342\n2911\n443\n13.0\n1.0\n6.3\n0.7\n28069\n733\n10.0\n0.7\n18.7\n0.1\n22.2\n0.0\n13.9\n0.7\n6.0\n0.8\n16.8\n0.1\n0.8\n0.3\n18.3\n0.9\n11.9\n0.9\n1.3\n0.3\n3.1\n0.5\n1.5\n0.2\n0.3317\n0.4241\n0.2448\n0.2999\n1.3005\nNaN\n0.6437\n0.4476\n0.3521\n0.1493\n1.5928\nNaN\n0.5253\n0.5282\n1.0535\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n1\n23255\n1817\n11.8\n0.9\n197683\n\n\n2\n3074\n1\nALABAMA\nAL\nBarbour\n1005\nBarbour County, Alabama\n884.875776\n26614\n0\n11802\n101\n9122\n286\n6235\n636\n1323\n267\n17249\n822\n4824\n362\n4399\n29\n5735\n19\n5086\n397\n1132\n160.1\n14402\n154.0\n382\n153.7\n179\n90.4\n3534\n215\n158\n62.2\n889\n145\n2932\n244\n26.4\n2.7\n12.8\n2.6\n17249\n822\n26.2\n1.9\n16.5\n0.1\n21.5\n0.1\n21.5\n1.7\n12.4\n1.7\n54.1\n0.6\n1.5\n0.6\n1.5\n0.8\n29.9\n1.8\n1.7\n0.7\n9.7\n1.5\n11.0\n0.9\n0.9261\n0.9526\n0.9491\n0.9551\n3.7829\nNaN\n0.4174\n0.3617\n0.8937\n0.9296\n2.6024\nNaN\n0.9042\n0.6979\n1.6020\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n1\n1\n1\n4\n0\n0\n0\n1\n1\n1\n0\n1\n0\n1\n0\n0\n1\n2\n8\n3079\n385\n13.0\n1.6\n27321\n\n\n3\n2113\n1\nALABAMA\nAL\nBibb\n1007\nBibb County, Alabama\n622.582355\n22572\n0\n8972\n76\n7048\n352\n3390\n818\n643\n207\n18988\n1773\n3040\n374\n3360\n111\n4756\n44\n3039\n367\n516\n202.8\n5696\n21.0\n96\n106.7\n195\n86.6\n2549\n261\n22\n22.8\n443\n136\n2001\n190\n16.5\n4.0\n7.1\n2.3\n18988\n1773\n19.3\n2.3\n14.9\n0.5\n21.1\n0.2\n14.8\n1.8\n7.3\n2.9\n25.2\n0.1\n0.4\n0.5\n2.2\n1.0\n28.4\n3.0\n0.3\n0.3\n6.3\n1.9\n8.9\n0.8\n0.5536\n0.5425\n0.8819\n0.7781\n2.7561\nNaN\n0.2585\n0.3088\n0.4419\n0.3266\n1.3359\nNaN\n0.6450\n0.3553\n1.0003\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n2\n2\n1859\n400\n9.0\n1.9\n18756\n\n\n4\n1\n1\nALABAMA\nAL\nBlount\n1009\nBlount County, Alabama\n644.806508\n57704\n0\n23850\n59\n20619\n403\n9441\n963\n1367\n284\n21033\n689\n7882\n645\n9921\n123\n13601\n29\n8538\n663\n1614\n301.0\n7122\n147.0\n1018\n248.1\n190\n85.8\n5467\n429\n391\n130.6\n816\n198\n552\n131\n16.5\n1.7\n6.0\n1.2\n21033\n689\n20.0\n1.6\n17.2\n0.2\n23.6\n0.1\n14.9\n1.2\n7.8\n1.5\n12.3\n0.3\n1.9\n0.5\n0.8\n0.4\n22.9\n1.8\n1.9\n0.6\n4.0\n1.0\n1.0\n0.2\n0.5536\n0.3792\n0.7504\n0.8058\n2.4890\nNaN\n0.4909\n0.6466\n0.4527\n0.4018\n1.9920\nNaN\n0.4238\n0.7482\n1.1719\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n6388\n740\n11.2\n1.3\n42597\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3137\n2111\n56\nWYOMING\nWY\nSweetwater\n56037\nSweetwater County, Wyoming\n10426.722050\n44812\n0\n19102\n77\n16533\n387\n4868\n878\n1232\n221\n30945\n1039\n2646\n345\n4220\n84\n12344\n222\n5144\n539\n1630\n265.2\n9045\n20.0\n890\n219.0\n1150\n213.0\n4285\n335\n487\n141.8\n512\n147\n648\n123\n11.0\n2.0\n5.1\n0.9\n30945\n1039\n9.3\n1.2\n9.4\n0.2\n27.5\n0.5\n11.6\n1.2\n9.9\n1.6\n20.2\n0.0\n2.1\n0.5\n6.0\n1.1\n22.4\n1.8\n2.9\n0.9\n3.1\n0.9\n1.4\n0.3\n0.2050\n0.2588\n0.1258\n0.2477\n0.8373\nNaN\n0.0197\n0.9309\n0.1614\n0.7256\n1.8376\nNaN\n0.5762\n0.7765\n1.3528\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n5861\n663\n13.2\n1.5\n47916\n\n\n3138\n1338\n56\nWYOMING\nWY\nTeton\n56039\nTeton County, Wyoming\n3995.936355\n22623\n0\n13292\n100\n8576\n448\n1650\n505\n311\n159\n46499\n4086\n784\n263\n2678\n52\n4411\n194\n1411\n335\n519\n201.5\n4253\n14.0\n776\n273.4\n871\n241.7\n403\n133\n314\n147.3\n147\n96\n833\n243\n7.3\n2.2\n2.1\n1.1\n46499\n4086\n4.6\n1.6\n11.8\n0.2\n19.5\n0.9\n6.3\n1.5\n6.1\n2.3\n18.8\n0.1\n3.6\n1.3\n6.6\n1.8\n3.0\n1.0\n3.7\n1.7\n1.7\n1.1\n3.7\n1.1\n0.0455\n0.0376\n0.0073\n0.0166\n0.1070\nNaN\n0.0837\n0.1579\n0.0032\n0.1573\n0.4021\nNaN\n0.5549\n0.8714\n1.4263\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3313\n627\n14.7\n2.8\n28921\n\n\n3139\n2112\n56\nWYOMING\nWY\nUinta\n56041\nUinta County, Wyoming\n2081.827988\n20893\n0\n8768\n34\n7432\n267\n3462\n750\n647\n238\n25636\n1972\n1351\n341\n2231\n50\n6176\n0\n3018\n368\n676\n212.0\n2566\n21.0\n257\n176.7\n540\n194.2\n2158\n289\n255\n144.5\n262\n115\n244\n85\n16.8\n3.6\n6.4\n2.4\n25636\n1972\n10.4\n2.6\n10.7\n0.2\n29.6\n0.0\n14.6\n1.8\n9.1\n2.8\n12.3\n0.1\n1.3\n0.9\n6.2\n2.2\n24.6\n3.3\n3.4\n1.9\n3.5\n1.5\n1.2\n0.4\n0.5743\n0.4368\n0.3980\n0.3292\n1.7383\nNaN\n0.0446\n0.9669\n0.4212\n0.6170\n2.0497\nNaN\n0.4212\n0.6625\n1.0837\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n3247\n488\n15.7\n2.4\n20530\n\n\n3140\n1339\n56\nWYOMING\nWY\nWashakie\n56043\nWashakie County, Wyoming\n2238.550443\n8351\n0\n3811\n32\n3493\n109\n1198\n298\n304\n91\n26325\n1892\n734\n162\n1641\n29\n2072\n74\n1552\n222\n257\n94.9\n1470\n56.0\n52\n66.4\n69\n42.2\n432\n108\n75\n41.9\n222\n87\n176\n63\n14.7\n3.7\n7.4\n2.2\n26325\n1892\n12.6\n2.8\n19.7\n0.3\n24.8\n0.9\n19.0\n2.7\n7.4\n2.7\n17.6\n0.7\n0.7\n0.8\n1.8\n1.1\n11.3\n2.8\n2.1\n1.2\n6.4\n2.5\n2.1\n0.8\n0.4362\n0.5813\n0.3473\n0.4890\n1.8539\nNaN\n0.7300\n0.8007\n0.7692\n0.3333\n2.6332\nNaN\n0.5361\n0.4642\n1.0003\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1381\n280\n16.9\n3.4\n8108\n\n\n3141\n1340\n56\nWYOMING\nWY\nWeston\n56045\nWeston County, Wyoming\n2398.090296\n7175\n0\n3498\n45\n3134\n121\n1002\n354\n124\n81\n29493\n2063\n445\n129\n1317\n82\n1511\n85\n957\n179\n207\n81.0\n558\n16.0\n78\n84.9\n115\n67.4\n832\n129\n33\n34.5\n164\n74\n356\n84\n14.7\n5.2\n3.6\n2.4\n29493\n2063\n8.6\n2.5\n18.4\n1.1\n21.1\n1.2\n14.0\n2.6\n6.6\n2.6\n7.8\n0.2\n1.1\n1.2\n3.3\n1.9\n23.8\n3.7\n1.1\n1.1\n5.2\n2.3\n5.0\n1.2\n0.4362\n0.1127\n0.1767\n0.1999\n0.9255\nNaN\n0.6151\n0.3075\n0.3610\n0.2213\n1.5049\nNaN\n0.2888\n0.6202\n0.9089\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n972\n221\n14.3\n3.2\n5915\n\n\n\n\n3142 rows × 124 columns\n\n\n\n\n\nsvi.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3142 entries, 0 to 3141\nColumns: 124 entries, FID to E_DAYPOP\ndtypes: float64(61), int64(59), object(4)\nmemory usage: 3.0+ MB\n\n\n\nsvi.isnull().sum()\n\nFID           0\nST            0\nSTATE         0\nST_ABBR       0\nCOUNTY        0\n             ..\nE_UNINSUR     0\nM_UNINSUR     0\nEP_UNINSUR    0\nMP_UNINSUR    0\nE_DAYPOP      0\nLength: 124, dtype: int64\n\n\n\nsvi.EP_MUNIT.describe()\n\ncount    3142.000000\nmean        4.612412\nstd         5.578143\nmin         0.000000\n25%         1.400000\n50%         2.900000\n75%         5.700000\nmax        89.600000\nName: EP_MUNIT, dtype: float64",
    "crumbs": [
      "Topical",
      "Comparing Indices: CDC's SVI and HVRI's SoVI"
    ]
  },
  {
    "objectID": "Topical/sovi_svi.html#cdc-svi",
    "href": "Topical/sovi_svi.html#cdc-svi",
    "title": "Comparing Indices: CDC’s SVI and HVRI’s SoVI",
    "section": "",
    "text": "import os\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nsvi = pd.read_excel('Assignment/SVI2016_US_COUNTY_Assignment.xlsx')\nsvi\n\n\n\n\n\n\n\n\n\nFID\nST\nSTATE\nST_ABBR\nCOUNTY\nGEOID\nLOCATION\nAREA_SQMI\nE_TOTPOP\nM_TOTPOP\nE_HU\nM_HU\nE_HH\nM_HH\nE_POV\nM_POV\nE_UNEMP\nM_UNEMP\nE_PCI\nM_PCI\nE_NOHSDP\nM_NOHSDP\nE_AGE65\nM_AGE65\nE_AGE17\nM_AGE17\nE_DISABL\nM_DISABL\nE_SNGPNT\nM_SNGPNT\nE_MINRTY\nM_MINRTY\nE_LIMENG\nM_LIMENG\nE_MUNIT\nM_MUNIT\nE_MOBILE\nM_MOBILE\nE_CROWD\nM_CROWD\nE_NOVEH\nM_NOVEH\nE_GROUPQ\nM_GROUPQ\nEP_POV\nMP_POV\nEP_UNEMP\nMP_UNEMP\nEP_PCI\nMP_PCI\nEP_NOHSDP\nMP_NOHSDP\nEP_AGE65\nMP_AGE65\nEP_AGE17\nMP_AGE17\nEP_DISABL\nMP_DISABL\nEP_SNGPNT\nMP_SNGPNT\nEP_MINRTY\nMP_MINRTY\nEP_LIMENG\nMP_LIMENG\nEP_MUNIT\nMP_MUNIT\nEP_MOBILE\nMP_MOBILE\nEP_CROWD\nMP_CROWD\nEP_NOVEH\nMP_NOVEH\nEP_GROUPQ\nMP_GROUPQ\nEPL_POV\nEPL_UNEMP\nEPL_PCI\nEPL_NOHSDP\nSPL_THEME1\nRPL_THEME1\nEPL_AGE65\nEPL_AGE17\nEPL_DISABL\nEPL_SNGPNT\nSPL_THEME2\nRPL_THEME2\nEPL_MINRTY\nEPL_LIMENG\nSPL_THEME3\nRPL_THEME3\nEPL_MUNIT\nEPL_MOBILE\nEPL_CROWD\nEPL_NOVEH\nEPL_GROUPQ\nSPL_THEME4\nRPL_THEME4\nSPL_THEMES\nRPL_THEMES\nF_POV\nF_UNEMP\nF_PCI\nF_NOHSDP\nF_THEME1\nF_AGE65\nF_AGE17\nF_DISABL\nF_SNGPNT\nF_THEME2\nF_MINRTY\nF_LIMENG\nF_THEME3\nF_MUNIT\nF_MOBILE\nF_CROWD\nF_NOVEH\nF_GROUPQ\nF_THEME4\nF_TOTAL\nE_UNINSUR\nM_UNINSUR\nEP_UNINSUR\nMP_UNINSUR\nE_DAYPOP\n\n\n\n\n0\n0\n1\nALABAMA\nAL\nAutauga\n1001\nAutauga County, Alabama\n594.446120\n55049\n0\n22714\n75\n20800\n391\n6697\n1037\n1437\n277\n26168\n1221\n4528\n445\n7695\n104\n13853\n34\n10009\n850\n1516\n267.0\n13386\n161.0\n432\n163.3\n1034\n329.9\n4095\n379\n254\n104.5\n1024\n242\n490\n163\n12.3\n1.9\n5.6\n1.1\n26168\n1221\n12.4\n1.2\n14.0\n0.2\n25.2\n0.1\n18.4\n1.6\n7.3\n1.3\n24.3\n0.3\n0.8\n0.3\n4.6\n1.5\n18.0\n1.7\n1.2\n0.5\n4.9\n1.1\n0.9\n0.3\n0.2824\n0.3298\n0.3607\n0.4744\n1.4473\nNaN\n0.1964\n0.8313\n0.7380\n0.3200\n2.0856\nNaN\n0.6339\n0.5355\n1.1694\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4852\n649\n8.9\n1.2\n40854\n\n\n1\n1341\n1\nALABAMA\nAL\nBaldwin\n1003\nBaldwin County, Alabama\n1589.807425\n199510\n0\n107579\n202\n75149\n1285\n25551\n1920\n5887\n674\n28069\n733\n13956\n974\n37338\n79\n44270\n0\n27390\n1430\n4494\n589.8\n33560\n245.0\n1540\n489.6\n19711\n936.9\n12829\n935\n958\n219.1\n2303\n342\n2911\n443\n13.0\n1.0\n6.3\n0.7\n28069\n733\n10.0\n0.7\n18.7\n0.1\n22.2\n0.0\n13.9\n0.7\n6.0\n0.8\n16.8\n0.1\n0.8\n0.3\n18.3\n0.9\n11.9\n0.9\n1.3\n0.3\n3.1\n0.5\n1.5\n0.2\n0.3317\n0.4241\n0.2448\n0.2999\n1.3005\nNaN\n0.6437\n0.4476\n0.3521\n0.1493\n1.5928\nNaN\n0.5253\n0.5282\n1.0535\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n1\n23255\n1817\n11.8\n0.9\n197683\n\n\n2\n3074\n1\nALABAMA\nAL\nBarbour\n1005\nBarbour County, Alabama\n884.875776\n26614\n0\n11802\n101\n9122\n286\n6235\n636\n1323\n267\n17249\n822\n4824\n362\n4399\n29\n5735\n19\n5086\n397\n1132\n160.1\n14402\n154.0\n382\n153.7\n179\n90.4\n3534\n215\n158\n62.2\n889\n145\n2932\n244\n26.4\n2.7\n12.8\n2.6\n17249\n822\n26.2\n1.9\n16.5\n0.1\n21.5\n0.1\n21.5\n1.7\n12.4\n1.7\n54.1\n0.6\n1.5\n0.6\n1.5\n0.8\n29.9\n1.8\n1.7\n0.7\n9.7\n1.5\n11.0\n0.9\n0.9261\n0.9526\n0.9491\n0.9551\n3.7829\nNaN\n0.4174\n0.3617\n0.8937\n0.9296\n2.6024\nNaN\n0.9042\n0.6979\n1.6020\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n1\n1\n1\n4\n0\n0\n0\n1\n1\n1\n0\n1\n0\n1\n0\n0\n1\n2\n8\n3079\n385\n13.0\n1.6\n27321\n\n\n3\n2113\n1\nALABAMA\nAL\nBibb\n1007\nBibb County, Alabama\n622.582355\n22572\n0\n8972\n76\n7048\n352\n3390\n818\n643\n207\n18988\n1773\n3040\n374\n3360\n111\n4756\n44\n3039\n367\n516\n202.8\n5696\n21.0\n96\n106.7\n195\n86.6\n2549\n261\n22\n22.8\n443\n136\n2001\n190\n16.5\n4.0\n7.1\n2.3\n18988\n1773\n19.3\n2.3\n14.9\n0.5\n21.1\n0.2\n14.8\n1.8\n7.3\n2.9\n25.2\n0.1\n0.4\n0.5\n2.2\n1.0\n28.4\n3.0\n0.3\n0.3\n6.3\n1.9\n8.9\n0.8\n0.5536\n0.5425\n0.8819\n0.7781\n2.7561\nNaN\n0.2585\n0.3088\n0.4419\n0.3266\n1.3359\nNaN\n0.6450\n0.3553\n1.0003\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n2\n2\n1859\n400\n9.0\n1.9\n18756\n\n\n4\n1\n1\nALABAMA\nAL\nBlount\n1009\nBlount County, Alabama\n644.806508\n57704\n0\n23850\n59\n20619\n403\n9441\n963\n1367\n284\n21033\n689\n7882\n645\n9921\n123\n13601\n29\n8538\n663\n1614\n301.0\n7122\n147.0\n1018\n248.1\n190\n85.8\n5467\n429\n391\n130.6\n816\n198\n552\n131\n16.5\n1.7\n6.0\n1.2\n21033\n689\n20.0\n1.6\n17.2\n0.2\n23.6\n0.1\n14.9\n1.2\n7.8\n1.5\n12.3\n0.3\n1.9\n0.5\n0.8\n0.4\n22.9\n1.8\n1.9\n0.6\n4.0\n1.0\n1.0\n0.2\n0.5536\n0.3792\n0.7504\n0.8058\n2.4890\nNaN\n0.4909\n0.6466\n0.4527\n0.4018\n1.9920\nNaN\n0.4238\n0.7482\n1.1719\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n6388\n740\n11.2\n1.3\n42597\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3137\n2111\n56\nWYOMING\nWY\nSweetwater\n56037\nSweetwater County, Wyoming\n10426.722050\n44812\n0\n19102\n77\n16533\n387\n4868\n878\n1232\n221\n30945\n1039\n2646\n345\n4220\n84\n12344\n222\n5144\n539\n1630\n265.2\n9045\n20.0\n890\n219.0\n1150\n213.0\n4285\n335\n487\n141.8\n512\n147\n648\n123\n11.0\n2.0\n5.1\n0.9\n30945\n1039\n9.3\n1.2\n9.4\n0.2\n27.5\n0.5\n11.6\n1.2\n9.9\n1.6\n20.2\n0.0\n2.1\n0.5\n6.0\n1.1\n22.4\n1.8\n2.9\n0.9\n3.1\n0.9\n1.4\n0.3\n0.2050\n0.2588\n0.1258\n0.2477\n0.8373\nNaN\n0.0197\n0.9309\n0.1614\n0.7256\n1.8376\nNaN\n0.5762\n0.7765\n1.3528\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n5861\n663\n13.2\n1.5\n47916\n\n\n3138\n1338\n56\nWYOMING\nWY\nTeton\n56039\nTeton County, Wyoming\n3995.936355\n22623\n0\n13292\n100\n8576\n448\n1650\n505\n311\n159\n46499\n4086\n784\n263\n2678\n52\n4411\n194\n1411\n335\n519\n201.5\n4253\n14.0\n776\n273.4\n871\n241.7\n403\n133\n314\n147.3\n147\n96\n833\n243\n7.3\n2.2\n2.1\n1.1\n46499\n4086\n4.6\n1.6\n11.8\n0.2\n19.5\n0.9\n6.3\n1.5\n6.1\n2.3\n18.8\n0.1\n3.6\n1.3\n6.6\n1.8\n3.0\n1.0\n3.7\n1.7\n1.7\n1.1\n3.7\n1.1\n0.0455\n0.0376\n0.0073\n0.0166\n0.1070\nNaN\n0.0837\n0.1579\n0.0032\n0.1573\n0.4021\nNaN\n0.5549\n0.8714\n1.4263\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3313\n627\n14.7\n2.8\n28921\n\n\n3139\n2112\n56\nWYOMING\nWY\nUinta\n56041\nUinta County, Wyoming\n2081.827988\n20893\n0\n8768\n34\n7432\n267\n3462\n750\n647\n238\n25636\n1972\n1351\n341\n2231\n50\n6176\n0\n3018\n368\n676\n212.0\n2566\n21.0\n257\n176.7\n540\n194.2\n2158\n289\n255\n144.5\n262\n115\n244\n85\n16.8\n3.6\n6.4\n2.4\n25636\n1972\n10.4\n2.6\n10.7\n0.2\n29.6\n0.0\n14.6\n1.8\n9.1\n2.8\n12.3\n0.1\n1.3\n0.9\n6.2\n2.2\n24.6\n3.3\n3.4\n1.9\n3.5\n1.5\n1.2\n0.4\n0.5743\n0.4368\n0.3980\n0.3292\n1.7383\nNaN\n0.0446\n0.9669\n0.4212\n0.6170\n2.0497\nNaN\n0.4212\n0.6625\n1.0837\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n3247\n488\n15.7\n2.4\n20530\n\n\n3140\n1339\n56\nWYOMING\nWY\nWashakie\n56043\nWashakie County, Wyoming\n2238.550443\n8351\n0\n3811\n32\n3493\n109\n1198\n298\n304\n91\n26325\n1892\n734\n162\n1641\n29\n2072\n74\n1552\n222\n257\n94.9\n1470\n56.0\n52\n66.4\n69\n42.2\n432\n108\n75\n41.9\n222\n87\n176\n63\n14.7\n3.7\n7.4\n2.2\n26325\n1892\n12.6\n2.8\n19.7\n0.3\n24.8\n0.9\n19.0\n2.7\n7.4\n2.7\n17.6\n0.7\n0.7\n0.8\n1.8\n1.1\n11.3\n2.8\n2.1\n1.2\n6.4\n2.5\n2.1\n0.8\n0.4362\n0.5813\n0.3473\n0.4890\n1.8539\nNaN\n0.7300\n0.8007\n0.7692\n0.3333\n2.6332\nNaN\n0.5361\n0.4642\n1.0003\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1381\n280\n16.9\n3.4\n8108\n\n\n3141\n1340\n56\nWYOMING\nWY\nWeston\n56045\nWeston County, Wyoming\n2398.090296\n7175\n0\n3498\n45\n3134\n121\n1002\n354\n124\n81\n29493\n2063\n445\n129\n1317\n82\n1511\n85\n957\n179\n207\n81.0\n558\n16.0\n78\n84.9\n115\n67.4\n832\n129\n33\n34.5\n164\n74\n356\n84\n14.7\n5.2\n3.6\n2.4\n29493\n2063\n8.6\n2.5\n18.4\n1.1\n21.1\n1.2\n14.0\n2.6\n6.6\n2.6\n7.8\n0.2\n1.1\n1.2\n3.3\n1.9\n23.8\n3.7\n1.1\n1.1\n5.2\n2.3\n5.0\n1.2\n0.4362\n0.1127\n0.1767\n0.1999\n0.9255\nNaN\n0.6151\n0.3075\n0.3610\n0.2213\n1.5049\nNaN\n0.2888\n0.6202\n0.9089\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n972\n221\n14.3\n3.2\n5915\n\n\n\n\n3142 rows × 124 columns\n\n\n\n\n\nsvi.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3142 entries, 0 to 3141\nColumns: 124 entries, FID to E_DAYPOP\ndtypes: float64(61), int64(59), object(4)\nmemory usage: 3.0+ MB\n\n\n\nsvi.isnull().sum()\n\nFID           0\nST            0\nSTATE         0\nST_ABBR       0\nCOUNTY        0\n             ..\nE_UNINSUR     0\nM_UNINSUR     0\nEP_UNINSUR    0\nMP_UNINSUR    0\nE_DAYPOP      0\nLength: 124, dtype: int64\n\n\n\nsvi.EP_MUNIT.describe()\n\ncount    3142.000000\nmean        4.612412\nstd         5.578143\nmin         0.000000\n25%         1.400000\n50%         2.900000\n75%         5.700000\nmax        89.600000\nName: EP_MUNIT, dtype: float64",
    "crumbs": [
      "Topical",
      "Comparing Indices: CDC's SVI and HVRI's SoVI"
    ]
  },
  {
    "objectID": "Topical/sovi_svi.html#total-combined-svi-values",
    "href": "Topical/sovi_svi.html#total-combined-svi-values",
    "title": "Comparing Indices: CDC’s SVI and HVRI’s SoVI",
    "section": "Total Combined SVI Values",
    "text": "Total Combined SVI Values\n\nsvi_geo.columns[svi_geo.columns.str.contains('THEME')]\n\nIndex(['SPL_THEME1', 'RPL_THEME1', 'SPL_THEME2', 'RPL_THEME2', 'SPL_THEME3',\n       'RPL_THEME3', 'SPL_THEME4', 'RPL_THEME4', 'SPL_THEMES', 'RPL_THEMES',\n       'F_THEME1', 'F_THEME2', 'F_THEME3', 'F_THEME4'],\n      dtype='object')\n\n\n\nsvi_geo['SPL_THEME1']\n\n0       1.4473\n1       1.3005\n2       3.7829\n3       2.7561\n4       2.4890\n         ...  \n3137    0.8373\n3138    0.1070\n3139    1.7383\n3140    1.8539\n3141    0.9255\nName: SPL_THEME1, Length: 3142, dtype: float64\n\n\n\nsvi_geo['Composite_Index'] = svi_geo[['SPL_THEME1', 'SPL_THEME2', 'SPL_THEME3', 'SPL_THEME4']].sum(axis=1)\n\nsvi_geo['RPL_THEMES'] = svi_geo['Composite_Index'].rank(pct=True)\n\nsvi_geo.plot(column='RPL_THEMES', legend=True, figsize=(10, 10))\nplt.title('Spatial Distribution of Composite SVI Index')\nplt.show()",
    "crumbs": [
      "Topical",
      "Comparing Indices: CDC's SVI and HVRI's SoVI"
    ]
  },
  {
    "objectID": "Topical/sovi_svi.html#sovi-by-hvri",
    "href": "Topical/sovi_svi.html#sovi-by-hvri",
    "title": "Comparing Indices: CDC’s SVI and HVRI’s SoVI",
    "section": "SOVI by HVRI",
    "text": "SOVI by HVRI\nI am using SoVI 2019 US County Data to compare with CDC’s SVI.\n\nsovi= pd.read_excel('sovi2019_countyus.xlsx')\nsovi.head()\n\n\n\n\n\n\n\n\n\nNo\nGeoID\nGeoID_A\nName\nState\nSoVI2019US\n\n\n\n\n0\n1\n0500000US01001\n0500000US01001\nAutauga County, Alabama\nAlabama\n-3.163005\n\n\n1\n2\n0500000US01003\n0500000US01003\nBaldwin County, Alabama\nAlabama\n-1.704175\n\n\n2\n3\n0500000US01005\n0500000US01005\nBarbour County, Alabama\nAlabama\n2.310552\n\n\n3\n4\n0500000US01007\n0500000US01007\nBibb County, Alabama\nAlabama\n-0.540243\n\n\n4\n5\n0500000US01009\n0500000US01009\nBlount County, Alabama\nAlabama\n-2.473869\n\n\n\n\n\n\n\n\n\nsovi['GOEID'] = sovi['GeoID'].str[-5:].astype(str)\nsovi['GOEID']\n\n0       01001\n1       01003\n2       01005\n3       01007\n4       01009\n        ...  \n3137    56037\n3138    56039\n3139    56041\n3140    56043\n3141    56045\nName: GOEID, Length: 3142, dtype: object\n\n\n\nsovi_geo= sovi.merge(us_rescaled[['GEOID', 'NAMELSAD', 'geometry']], left_on='GOEID', right_on='GEOID')\nsovi_geo= gpd.GeoDataFrame(sovi_geo, crs=us_rescaled.crs)\n\nText(0.5, 1.0, 'Spatial Distribution of SOVI Index')\n\n\n\n\n\n\n\n\n\n\nsovi_geo.plot(column='SoVI2019US',figsize=(10,10))\nplt.title('Spatial Distribution of SOVI Index')\n\nText(0.5, 1.0, 'Spatial Distribution of SOVI Index')",
    "crumbs": [
      "Topical",
      "Comparing Indices: CDC's SVI and HVRI's SoVI"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Adipisicing proident minim non non dolor quis. Pariatur in ipsum aliquip magna. Qui ad aliqua nulla excepteur dolor nostrud quis nisi. Occaecat proident eiusmod in cupidatat. Elit qui laboris sit aliquip proident dolore. Officia commodo commodo in eiusmod aliqua sint cupidatat consectetur aliqua sint reprehenderit.\nOccaecat incididunt esse et elit adipisicing sit est cupidatat consequat. Incididunt exercitation amet dolor non sit anim veniam veniam sint velit. Labore irure reprehenderit ut esse. Minim quis commodo nisi voluptate.",
    "crumbs": [
      "About me"
    ]
  },
  {
    "objectID": "Archive/4-folium.html",
    "href": "Archive/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "Archive/4-folium.html#finding-the-shortest-route",
    "href": "Archive/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "Archive/4-folium.html#examining-trash-related-311-requests",
    "href": "Archive/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "Archive/1-python-code-blocks.html",
    "href": "Archive/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "Archive/3-altair-hvplot.html",
    "href": "Archive/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "Archive/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "Archive/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "Archive/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "Archive/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "Spatial SQL/raster_function.html",
    "href": "Spatial SQL/raster_function.html",
    "title": "Working with Raster Data in PostGIS",
    "section": "",
    "text": "Disclaimer: This guide is based on the Spatial SQL book by Matthew Forrest.\nIn this exercise, we’ll guide you through the process of working with raster data in PostGIS. This involves using various tools and interfaces to ingest, analyze, and visualize raster data. Here’s an explicit delineation of the relationships between the technologies we’ll use and the steps involved, flagged by the command line interface or application where each step is performed:\n\n\n\nPostgreSQL/PostGIS: An open-source relational database system enhanced with PostGIS, an extension for spatial data support. PostGIS allows for storing and querying data with geographic properties.\nDocker: A platform for developing, shipping, and running applications in containers. Containers allow you to package your application, including its dependencies, into a standardized unit for software development.\npgAdmin: A web-based administration tool for PostgreSQL. It provides a graphical interface to manage databases, run queries, and manage database objects.\nQGIS: An open-source geographic information system application that allows you to create, edit, analyze, and visualize geospatial information on Windows, Mac, Linux, and BSD.\nRemote Server: In this context, it refers to a computer that hosts your PostGIS database. It can be accessed over a network via tools like pgAdmin or command-line interfaces.\nYour Local Machine: The computer you’re using to perform this exercise. It interacts with Docker to run PostGIS in a container and uses pgAdmin and QGIS for database management and geospatial analysis, respectively.\n\nSpatial analytics often relies on both vector and raster data types. While PostGIS has been traditionally used for vector data, its capabilities now extend to handling raster data efficiently, thanks to extensions added in versions following PostGIS 1.5. This guide walks you through the process of ingesting, analyzing, and transforming raster data using PostGIS and various tools including Docker, pgAdmin, and QGIS.\n\n\n\n\n\n\nInterface: Terminal on your local machine.\nCommand: Use docker pull to download the PostGIS Docker image and docker run to start a PostGIS container. This step encapsulates both PostgreSQL and PostGIS in a containerized environment, simplifying the setup process.\n\nPull the PostGIS Docker Image:\n\ndocker pull postgis/postgis:15-master\n\nRun the Docker Container:\n\ndocker run --name mini-postgis -p 35432:5432 --network=\"host\" \\\n-v /your/data/directory:/mnt/mydata \\\n-e POSTGRES_USER=admin -e POSTGRES_PASSWORD=password -d postgis/postgis:15-master\nThis command creates a container named mini-postgis with the PostGIS image, mapping port 35432 to the container’s port 5432, and setting the Postgres user and password to admin and password, respectively. The -v flag mounts the host directory /your/data/directory to the container’s /mnt/mydata directory. The --network=\"host\" flag allows the container to use the host’s network settings. Here, we will use “mini-postgis” docker container and “postgis” as the local database.\n\n\n\n\n\nInterface: Terminal on your local machine.\nContainer: mini-postgis.\nCommand: Use docker exec -it mini-postgis bash to access the PostGIS container’s command line. This step is necessary for running the raster2pgsql command to ingest raster data into PostGIS.\nThe access command may not be necessary if you haven’t closed the terminal after running the Docker container. In that case, you can proceed to the next step without accessing the container again.\n\n\n\n\n\nInterface: Terminal within the mini-postgis Docker container.\nCommand: Run raster2pgsql followed by psql to ingest raster data. This process converts raster data into a format that can be stored in PostGIS and then inserts it into the database.\nraster2pgsql -s 4326 -I -C -M /mnt/mydata/raster.tif -F -t 100x100 | psql -d postgis -h 127.0.0.1 -p 25432 -U docker -W\nThis command ingests the raster data from /mnt/mydata/raster.tif into the database postgis. The -s flag specifies the spatial reference system (SRS) of the raster data, -I creates a spatial index, -C cleans up the data after ingestion, and -M uses the raster column type. The -F flag forces the data to be ingested, and -t specifies the tile size. The tile size should be adjusted based on the raster data’s resolution and size. The -d flag specifies the database name, -h specifies the host, -p specifies the port, and -U specifies the user. The -W flag tells the command to skip the password prompt.\n\n\n\n\n\nInterface: pgAdmin.\nAction: Connect to your main database and run CREATE EXTENSION postgis_raster; to enable the postgis_raster extension. This extension provides functions for working with raster data in PostGIS.\nCREATE EXTENSION postgis_raster;\nThis command enables the postgis_raster extension in your database, allowing you to work with raster data.\n\n\n\n\n\nInterface: pgAdmin.\nAction: Use the ST_Contour function to generate contour lines from raster data. This function creates contour lines at specified intervals based on the raster data.\nwith c as (\n    select\n    (st_contour(rast, 1, 200.00)).*\n    from\n    raster_table\n    where\n    filename = 'raster.tif'\n)\nselect\n    st_transform(geom, 4326),\n    id,\n    value\nfrom\n    c\nThis query generates contour lines from the raster data in the raster_table table, where the filename is raster.tif. The ST_Contour function creates contour lines at intervals of 200.\n\n\n\n\n\nInterface: QGIS.\n\n\n\n\n\nTo ingest raster data into a PostGIS database, we’ll start by using the raster2pgsql tool. This process involves creating a Docker image specifically for raster data ingestion, as running the ingestion commands in the same environment as your main database isn’t considered best practice.\n\nPull the PostGIS Docker Image:\ndocker pull postgis/postgis:15-master\nAlternatively, you can use the Docker Desktop app to search for the postgis/postgis image and pull the 15-master tag.\nRun the Docker Container:\nAfter pulling the image, run it using the following command, adjusting the volume path to match your system:\ndocker run --name mini-postgis -p 35432:5432 --network=\"host\" \\\n-v /your/data/directory:/mnt/mydata \\\n-e POSTGRES_USER=admin -e POSTGRES_PASSWORD=password -d postgis/postgis:15-master\nThis command creates a container named mini-postgis with the PostGIS image, mapping port 35432 to the container’s port 5432, and setting the Postgres user and password to admin and password, respectively. The -v flag mounts the host directory /your/data/directory to the container’s /mnt/mydata directory.\nEnable the postgis_raster Extension:\nUsing pgAdmin, connect to your main database and run:\nCREATE EXTENSION postgis_raster;\nThis command enables the postgis_raster extension in your database, allowing you to work with raster data.\nIngest Raster Data:\nTo ingest raster data into your database, run the following command in the Docker container, mini-postgis.\nIf you have not closed the terminal, you can run the following command in the same terminal; otherwise, open a new terminal and access the container using docker exec -it mini-postgis bash. Then run the following command:\nraster2pgsql -s 4326 -I -C -M /mnt/mydata/raster.tif -F -t 100x100 | psql -h 127.0.0.1 -p 25432 -U admin -d your_database\nThis command ingests the raster data from /mnt/mydata/raster.tif into the database your_database. The -s flag specifies the spatial reference system (SRS) of the raster data, -I creates a spatial index, -C cleans up the data after ingestion, and -M uses the raster column type. The -F flag forces the data to be ingested, and -t specifies the tile size. The tile size should be adjusted based on the raster data’s resolution and size.\nImportant Note: The port 25432 is used for the database connection within this command, which should match the port number where your primary PostGIS database is accessible. This is distinct from the 35432 port used by the mini-postgis Docker container itself. Make sure to use the correct port numbers based on how your Docker containers and databases are configured.\nAnalyze the Raster Data:\n\n\nInterface: pgAdmin.\nAction: After ingesting the raster data, you can analyze it using PostGIS functions. For example, you can generate contour lines from the raster data using the ST_Contour function.\n```sql\nwith c as (\n    select\n    (st_contour(rast, 1, 200.00)).*\n    from\n    raster_table\n    where\n    filename = 'raster.tif'\n)\nselect\n    st_transform(geom, 4326),\n    id,\n    value\nfrom\n    c\n```\nThis query generates contour lines from the raster data in the raster_table table, where the filename is raster.tif. The ST_Contour function creates contour lines at intervals of 200.\n\n\nVisualize the Contour Lines:\n\n\nInterface: QGIS.\nAction: To visualize the contour lines in QGIS, connect to your PostGIS database and add the contours table as a layer. You can adjust the symbology and labeling to better visualize the contour lines.\n\n\n\nraster_contour_before\n\n\n\n\n\nraster_contour_before\n\n\n\n\n\n\n\nPostGIS provides functions for interpolating raster data, such as ST_Reclass and ST_MapAlgebraExpr. These functions can be used to perform various raster operations, such as reclassification and map algebra.\n\n\n\nInterface: pgAdmin\nAction: Import weather station data into PostGIS using ogr2ogr via pgAdmin. Run SQL scripts to perform IDW spatial interpolation on the imported weather data, creating a new interpolated raster within PostGIS.\nCommand: ```sql – Import weather station data into PostGIS CREATE TABLE weather_stations AS SELECT ST_SetSRID(ST_MakePoint(lon, lat), 4326) AS geom, temperature FROM weather_stations_csv;\n– Perform IDW spatial interpolation CREATE TABLE interpolated_raster AS SELECT ST_MapAlgebraExpr( ST_AddBand(ST_MakeEmptyRaster(0.1, 0.1, 0, 0, 0.1), ‘32BF’::text, 1, 0), ‘32BF’, ‘ST_InvDistWeighting(rast, temperature, 12, 1, 0.5, 0.1, 0.1, 0.1, 0.1, 12, 0, false)’, ‘32BF’, ‘LAST’ ) AS rast FROM weather_stations; ```\n\nThis script imports weather station data from a CSV file into a PostGIS table named weather_stations. It then performs Inverse Distance Weighting (IDW) spatial interpolation on the temperature values from the weather stations, creating an interpolated raster in the interpolated_raster table.",
    "crumbs": [
      "Spatial SQL/index.qmd",
      "Working with Raster Data in PostGIS"
    ]
  },
  {
    "objectID": "Spatial SQL/raster_function.html#introduction-to-the-technologies",
    "href": "Spatial SQL/raster_function.html#introduction-to-the-technologies",
    "title": "Working with Raster Data in PostGIS",
    "section": "",
    "text": "PostgreSQL/PostGIS: An open-source relational database system enhanced with PostGIS, an extension for spatial data support. PostGIS allows for storing and querying data with geographic properties.\nDocker: A platform for developing, shipping, and running applications in containers. Containers allow you to package your application, including its dependencies, into a standardized unit for software development.\npgAdmin: A web-based administration tool for PostgreSQL. It provides a graphical interface to manage databases, run queries, and manage database objects.\nQGIS: An open-source geographic information system application that allows you to create, edit, analyze, and visualize geospatial information on Windows, Mac, Linux, and BSD.\nRemote Server: In this context, it refers to a computer that hosts your PostGIS database. It can be accessed over a network via tools like pgAdmin or command-line interfaces.\nYour Local Machine: The computer you’re using to perform this exercise. It interacts with Docker to run PostGIS in a container and uses pgAdmin and QGIS for database management and geospatial analysis, respectively.\n\nSpatial analytics often relies on both vector and raster data types. While PostGIS has been traditionally used for vector data, its capabilities now extend to handling raster data efficiently, thanks to extensions added in versions following PostGIS 1.5. This guide walks you through the process of ingesting, analyzing, and transforming raster data using PostGIS and various tools including Docker, pgAdmin, and QGIS.",
    "crumbs": [
      "Spatial SQL/index.qmd",
      "Working with Raster Data in PostGIS"
    ]
  },
  {
    "objectID": "Spatial SQL/raster_function.html#basic-raster-analysis",
    "href": "Spatial SQL/raster_function.html#basic-raster-analysis",
    "title": "Working with Raster Data in PostGIS",
    "section": "",
    "text": "Interface: Terminal on your local machine.\nCommand: Use docker pull to download the PostGIS Docker image and docker run to start a PostGIS container. This step encapsulates both PostgreSQL and PostGIS in a containerized environment, simplifying the setup process.\n\nPull the PostGIS Docker Image:\n\ndocker pull postgis/postgis:15-master\n\nRun the Docker Container:\n\ndocker run --name mini-postgis -p 35432:5432 --network=\"host\" \\\n-v /your/data/directory:/mnt/mydata \\\n-e POSTGRES_USER=admin -e POSTGRES_PASSWORD=password -d postgis/postgis:15-master\nThis command creates a container named mini-postgis with the PostGIS image, mapping port 35432 to the container’s port 5432, and setting the Postgres user and password to admin and password, respectively. The -v flag mounts the host directory /your/data/directory to the container’s /mnt/mydata directory. The --network=\"host\" flag allows the container to use the host’s network settings. Here, we will use “mini-postgis” docker container and “postgis” as the local database.\n\n\n\n\n\nInterface: Terminal on your local machine.\nContainer: mini-postgis.\nCommand: Use docker exec -it mini-postgis bash to access the PostGIS container’s command line. This step is necessary for running the raster2pgsql command to ingest raster data into PostGIS.\nThe access command may not be necessary if you haven’t closed the terminal after running the Docker container. In that case, you can proceed to the next step without accessing the container again.\n\n\n\n\n\nInterface: Terminal within the mini-postgis Docker container.\nCommand: Run raster2pgsql followed by psql to ingest raster data. This process converts raster data into a format that can be stored in PostGIS and then inserts it into the database.\nraster2pgsql -s 4326 -I -C -M /mnt/mydata/raster.tif -F -t 100x100 | psql -d postgis -h 127.0.0.1 -p 25432 -U docker -W\nThis command ingests the raster data from /mnt/mydata/raster.tif into the database postgis. The -s flag specifies the spatial reference system (SRS) of the raster data, -I creates a spatial index, -C cleans up the data after ingestion, and -M uses the raster column type. The -F flag forces the data to be ingested, and -t specifies the tile size. The tile size should be adjusted based on the raster data’s resolution and size. The -d flag specifies the database name, -h specifies the host, -p specifies the port, and -U specifies the user. The -W flag tells the command to skip the password prompt.\n\n\n\n\n\nInterface: pgAdmin.\nAction: Connect to your main database and run CREATE EXTENSION postgis_raster; to enable the postgis_raster extension. This extension provides functions for working with raster data in PostGIS.\nCREATE EXTENSION postgis_raster;\nThis command enables the postgis_raster extension in your database, allowing you to work with raster data.\n\n\n\n\n\nInterface: pgAdmin.\nAction: Use the ST_Contour function to generate contour lines from raster data. This function creates contour lines at specified intervals based on the raster data.\nwith c as (\n    select\n    (st_contour(rast, 1, 200.00)).*\n    from\n    raster_table\n    where\n    filename = 'raster.tif'\n)\nselect\n    st_transform(geom, 4326),\n    id,\n    value\nfrom\n    c\nThis query generates contour lines from the raster data in the raster_table table, where the filename is raster.tif. The ST_Contour function creates contour lines at intervals of 200.\n\n\n\n\n\nInterface: QGIS.\n\n\n\n\n\nTo ingest raster data into a PostGIS database, we’ll start by using the raster2pgsql tool. This process involves creating a Docker image specifically for raster data ingestion, as running the ingestion commands in the same environment as your main database isn’t considered best practice.\n\nPull the PostGIS Docker Image:\ndocker pull postgis/postgis:15-master\nAlternatively, you can use the Docker Desktop app to search for the postgis/postgis image and pull the 15-master tag.\nRun the Docker Container:\nAfter pulling the image, run it using the following command, adjusting the volume path to match your system:\ndocker run --name mini-postgis -p 35432:5432 --network=\"host\" \\\n-v /your/data/directory:/mnt/mydata \\\n-e POSTGRES_USER=admin -e POSTGRES_PASSWORD=password -d postgis/postgis:15-master\nThis command creates a container named mini-postgis with the PostGIS image, mapping port 35432 to the container’s port 5432, and setting the Postgres user and password to admin and password, respectively. The -v flag mounts the host directory /your/data/directory to the container’s /mnt/mydata directory.\nEnable the postgis_raster Extension:\nUsing pgAdmin, connect to your main database and run:\nCREATE EXTENSION postgis_raster;\nThis command enables the postgis_raster extension in your database, allowing you to work with raster data.\nIngest Raster Data:\nTo ingest raster data into your database, run the following command in the Docker container, mini-postgis.\nIf you have not closed the terminal, you can run the following command in the same terminal; otherwise, open a new terminal and access the container using docker exec -it mini-postgis bash. Then run the following command:\nraster2pgsql -s 4326 -I -C -M /mnt/mydata/raster.tif -F -t 100x100 | psql -h 127.0.0.1 -p 25432 -U admin -d your_database\nThis command ingests the raster data from /mnt/mydata/raster.tif into the database your_database. The -s flag specifies the spatial reference system (SRS) of the raster data, -I creates a spatial index, -C cleans up the data after ingestion, and -M uses the raster column type. The -F flag forces the data to be ingested, and -t specifies the tile size. The tile size should be adjusted based on the raster data’s resolution and size.\nImportant Note: The port 25432 is used for the database connection within this command, which should match the port number where your primary PostGIS database is accessible. This is distinct from the 35432 port used by the mini-postgis Docker container itself. Make sure to use the correct port numbers based on how your Docker containers and databases are configured.\nAnalyze the Raster Data:\n\n\nInterface: pgAdmin.\nAction: After ingesting the raster data, you can analyze it using PostGIS functions. For example, you can generate contour lines from the raster data using the ST_Contour function.\n```sql\nwith c as (\n    select\n    (st_contour(rast, 1, 200.00)).*\n    from\n    raster_table\n    where\n    filename = 'raster.tif'\n)\nselect\n    st_transform(geom, 4326),\n    id,\n    value\nfrom\n    c\n```\nThis query generates contour lines from the raster data in the raster_table table, where the filename is raster.tif. The ST_Contour function creates contour lines at intervals of 200.\n\n\nVisualize the Contour Lines:\n\n\nInterface: QGIS.\nAction: To visualize the contour lines in QGIS, connect to your PostGIS database and add the contours table as a layer. You can adjust the symbology and labeling to better visualize the contour lines.\n\n\n\nraster_contour_before\n\n\n\n\n\nraster_contour_before",
    "crumbs": [
      "Spatial SQL/index.qmd",
      "Working with Raster Data in PostGIS"
    ]
  },
  {
    "objectID": "Spatial SQL/raster_function.html#interpolation",
    "href": "Spatial SQL/raster_function.html#interpolation",
    "title": "Working with Raster Data in PostGIS",
    "section": "",
    "text": "PostGIS provides functions for interpolating raster data, such as ST_Reclass and ST_MapAlgebraExpr. These functions can be used to perform various raster operations, such as reclassification and map algebra.\n\n\n\nInterface: pgAdmin\nAction: Import weather station data into PostGIS using ogr2ogr via pgAdmin. Run SQL scripts to perform IDW spatial interpolation on the imported weather data, creating a new interpolated raster within PostGIS.\nCommand: ```sql – Import weather station data into PostGIS CREATE TABLE weather_stations AS SELECT ST_SetSRID(ST_MakePoint(lon, lat), 4326) AS geom, temperature FROM weather_stations_csv;\n– Perform IDW spatial interpolation CREATE TABLE interpolated_raster AS SELECT ST_MapAlgebraExpr( ST_AddBand(ST_MakeEmptyRaster(0.1, 0.1, 0, 0, 0.1), ‘32BF’::text, 1, 0), ‘32BF’, ‘ST_InvDistWeighting(rast, temperature, 12, 1, 0.5, 0.1, 0.1, 0.1, 0.1, 12, 0, false)’, ‘32BF’, ‘LAST’ ) AS rast FROM weather_stations; ```\n\nThis script imports weather station data from a CSV file into a PostGIS table named weather_stations. It then performs Inverse Distance Weighting (IDW) spatial interpolation on the temperature values from the weather stations, creating an interpolated raster in the interpolated_raster table.",
    "crumbs": [
      "Spatial SQL/index.qmd",
      "Working with Raster Data in PostGIS"
    ]
  },
  {
    "objectID": "Spatial SQL/index.html#introduction-to-spatial-sql",
    "href": "Spatial SQL/index.html#introduction-to-spatial-sql",
    "title": "Table of Contents for Spatial SQL Blog Series",
    "section": "Introduction to Spatial SQL",
    "text": "Introduction to Spatial SQL\n\nWhat is Spatial SQL?\nImportance of Spatial Data in Today’s World\nOverview of Spatial Databases (PostGIS, Microsoft SQL Server, Oracle Spatial, etc.)"
  },
  {
    "objectID": "Spatial SQL/index.html#setting-up-your-spatial-database-environment",
    "href": "Spatial SQL/index.html#setting-up-your-spatial-database-environment",
    "title": "Table of Contents for Spatial SQL Blog Series",
    "section": "Setting Up Your Spatial Database Environment",
    "text": "Setting Up Your Spatial Database Environment\n\nInstalling PostGIS with PostgreSQL (as an example)\nBasic Configuration for Spatial Data Handling\nTools and IDEs for Spatial SQL Development"
  },
  {
    "objectID": "Spatial SQL/index.html#understanding-spatial-data-types",
    "href": "Spatial SQL/index.html#understanding-spatial-data-types",
    "title": "Table of Contents for Spatial SQL Blog Series",
    "section": "Understanding Spatial Data Types",
    "text": "Understanding Spatial Data Types\n\nGeometry vs. Geography Data Types\nPoints, Lines, and Polygons: When to Use Each\nExploring Multi-geometry Types"
  },
  {
    "objectID": "Spatial SQL/index.html#basic-spatial-operations",
    "href": "Spatial SQL/index.html#basic-spatial-operations",
    "title": "Table of Contents for Spatial SQL Blog Series",
    "section": "Basic Spatial Operations",
    "text": "Basic Spatial Operations\n\nCreating Spatial Data: INSERTs and UPDATEs\nQuerying Spatial Data: SELECT Statements with Spatial Filters\nUnderstanding Spatial Functions: ST_Contains, ST_Intersects, and More"
  },
  {
    "objectID": "Spatial SQL/index.html#indexing-spatial-data",
    "href": "Spatial SQL/index.html#indexing-spatial-data",
    "title": "Table of Contents for Spatial SQL Blog Series",
    "section": "Indexing Spatial Data",
    "text": "Indexing Spatial Data\n\nThe Importance of Indexing in Spatial Databases\nCreating and Managing Spatial Indexes\nPerformance Comparison: With and Without Indexes"
  },
  {
    "objectID": "Spatial SQL/index.html#advanced-spatial-analysis",
    "href": "Spatial SQL/index.html#advanced-spatial-analysis",
    "title": "Table of Contents for Spatial SQL Blog Series",
    "section": "Advanced Spatial Analysis",
    "text": "Advanced Spatial Analysis\n\nSpatial Joins: Concepts and Queries\nAggregating Spatial Data: Summarizing Geographical Insights\nProximity Analysis: Finding Nearby Points of Interest"
  },
  {
    "objectID": "Spatial SQL/index.html#visualizing-spatial-data",
    "href": "Spatial SQL/index.html#visualizing-spatial-data",
    "title": "Table of Contents for Spatial SQL Blog Series",
    "section": "Visualizing Spatial Data",
    "text": "Visualizing Spatial Data\n\nTools and Libraries for Spatial Data Visualization\nIntegrating Spatial Visualizations in Web Applications\nCase Studies: Real-world Examples of Spatial Data Visualization"
  },
  {
    "objectID": "Spatial SQL/index.html#spatial-data-in-real-world-applications",
    "href": "Spatial SQL/index.html#spatial-data-in-real-world-applications",
    "title": "Table of Contents for Spatial SQL Blog Series",
    "section": "Spatial Data in Real World Applications",
    "text": "Spatial Data in Real World Applications\n\nUrban Planning and Management\nEnvironmental Monitoring and Conservation\nTransportation and Logistics Optimization"
  },
  {
    "objectID": "Spatial SQL/index.html#optimizing-spatial-queries",
    "href": "Spatial SQL/index.html#optimizing-spatial-queries",
    "title": "Table of Contents for Spatial SQL Blog Series",
    "section": "Optimizing Spatial Queries",
    "text": "Optimizing Spatial Queries\n\nBest Practices for Writing Efficient Spatial SQL Queries\nCommon Pitfalls in Spatial Query Performance\nCase Studies: Performance Improvements in Real-life Applications"
  },
  {
    "objectID": "Spatial SQL/index.html#future-of-spatial-sql-and-emerging-technologies",
    "href": "Spatial SQL/index.html#future-of-spatial-sql-and-emerging-technologies",
    "title": "Table of Contents for Spatial SQL Blog Series",
    "section": "Future of Spatial SQL and Emerging Technologies",
    "text": "Future of Spatial SQL and Emerging Technologies\n\nIntegrating Spatial Data with Machine Learning Models\nThe Role of Spatial SQL in IoT and Big Data\nTrends and Predictions: The Evolution of Spatial Technologies"
  },
  {
    "objectID": "Spatial SQL/index.html#resources-and-community",
    "href": "Spatial SQL/index.html#resources-and-community",
    "title": "Table of Contents for Spatial SQL Blog Series",
    "section": "Resources and Community",
    "text": "Resources and Community\n\nRecommended Books, Online Courses, and Tutorials\nCommunities and Forums for Spatial SQL Enthusiasts\nHow to Contribute to the Spatial SQL Community"
  },
  {
    "objectID": "Spatial SQL/setting_up_db.html",
    "href": "Spatial SQL/setting_up_db.html",
    "title": "Setting up a Spatial Database with Your Local Machine",
    "section": "",
    "text": "In this tutorial, we will set up a spatial database using the PostGIS extension for PostgreSQL. We will use the psql command-line tool to create a new database, enable the PostGIS extension, and load spatial data into the database.\n\n\nFirst, you need to install PostgreSQL and PostGIS on your system. You can download the installer for your operating system from the official websites:\n\nPostgreSQL: https://www.postgresql.org/download/\nPostGIS: https://postgis.net/install/\nCode for installing PostgreSQL and PostGIS on Ubuntu:\n\nbrew update\nbrew install postgresql postgresql-contrib postgis\nbrew is used to install, update, and manage packages on macOS.\n\n\n\nWith PostgreSQL and PostGIS installed, you can now create a new database. Open Terminal and execute the following command:\ncreatedb spatial_db\nThis command creates a new database named spatial_db. Feel free to replace spatial_db with any name you prefer.\n\n\n\nTo enable the PostGIS extension in your newly created database, execute the following command in Terminal:\npsql -d spatial_db -c \"CREATE EXTENSION postgis;\"\nThis enables the PostGIS extension within the spatial_db database.\n\n\n\nTo load spatial data into your database, use the shp2pgsql tool for importing shapefiles into a PostGIS-enabled database.\nFor instance, to import a shapefile named ‘countries.shp’ into the ‘spatial_db’ database, run the following command:\n\nshp2pgsql -I -s 4326 countries.shp public.countries | psql -d spatial_db\nThis imports the ‘countries.shp’ shapefile into the ‘spatial_db’ database under the ‘public.countries’ table.\n\n\n\nYou can verify that the database setup is successful by connecting to the database using the psql command-line tool. Run the following command in the terminal:\npsql -d spatial_db\nThis command connects to the ‘spatial_db’ database. You can run SQL queries to verify that the PostGIS extension is enabled and the spatial data is loaded correctly.\nThat’s it! You have successfully set up a spatial database using the PostGIS extension for PostgreSQL. You can now use the database to perform spatial queries and analysis.\n\n\n\nPort: A port is a communication endpoint used to identify specific processes or network services on a computer. PostgreSQL’s default port is 5432. To connect to a database on a different port, you’d use the command:\npsql -h localhost -p 5433 -U postgres -d spatial_db\nUser: The default user for PostgreSQL is postgres. You can specify a different user using the -U flag:\npsql -U myuser -d spatial_db\nHost: The default host for PostgreSQL is localhost. If your database is hosted on a different server, you can specify the host using the -h flag:\npsql -h myhost -d spatial_db",
    "crumbs": [
      "Spatial SQL/index.qmd",
      "Setting up a Spatial Database with Your Local Machine"
    ]
  },
  {
    "objectID": "Spatial SQL/setting_up_db.html#step-1-install-postgresql-and-postgis",
    "href": "Spatial SQL/setting_up_db.html#step-1-install-postgresql-and-postgis",
    "title": "Setting up a Spatial Database with Your Local Machine",
    "section": "",
    "text": "First, you need to install PostgreSQL and PostGIS on your system. You can download the installer for your operating system from the official websites:\n\nPostgreSQL: https://www.postgresql.org/download/\nPostGIS: https://postgis.net/install/\nCode for installing PostgreSQL and PostGIS on Ubuntu:\n\nbrew update\nbrew install postgresql postgresql-contrib postgis\nbrew is used to install, update, and manage packages on macOS.",
    "crumbs": [
      "Spatial SQL/index.qmd",
      "Setting up a Spatial Database with Your Local Machine"
    ]
  },
  {
    "objectID": "Spatial SQL/setting_up_db.html#step-2-create-a-new-database",
    "href": "Spatial SQL/setting_up_db.html#step-2-create-a-new-database",
    "title": "Setting up a Spatial Database with Your Local Machine",
    "section": "",
    "text": "With PostgreSQL and PostGIS installed, you can now create a new database. Open Terminal and execute the following command:\ncreatedb spatial_db\nThis command creates a new database named spatial_db. Feel free to replace spatial_db with any name you prefer.",
    "crumbs": [
      "Spatial SQL/index.qmd",
      "Setting up a Spatial Database with Your Local Machine"
    ]
  },
  {
    "objectID": "Spatial SQL/setting_up_db.html#step-3-enable-postgis-extension",
    "href": "Spatial SQL/setting_up_db.html#step-3-enable-postgis-extension",
    "title": "Setting up a Spatial Database with Your Local Machine",
    "section": "",
    "text": "To enable the PostGIS extension in your newly created database, execute the following command in Terminal:\npsql -d spatial_db -c \"CREATE EXTENSION postgis;\"\nThis enables the PostGIS extension within the spatial_db database.",
    "crumbs": [
      "Spatial SQL/index.qmd",
      "Setting up a Spatial Database with Your Local Machine"
    ]
  },
  {
    "objectID": "Spatial SQL/setting_up_db.html#step-4-load-spatial-data",
    "href": "Spatial SQL/setting_up_db.html#step-4-load-spatial-data",
    "title": "Setting up a Spatial Database with Your Local Machine",
    "section": "",
    "text": "To load spatial data into your database, use the shp2pgsql tool for importing shapefiles into a PostGIS-enabled database.\nFor instance, to import a shapefile named ‘countries.shp’ into the ‘spatial_db’ database, run the following command:\n\nshp2pgsql -I -s 4326 countries.shp public.countries | psql -d spatial_db\nThis imports the ‘countries.shp’ shapefile into the ‘spatial_db’ database under the ‘public.countries’ table.",
    "crumbs": [
      "Spatial SQL/index.qmd",
      "Setting up a Spatial Database with Your Local Machine"
    ]
  },
  {
    "objectID": "Spatial SQL/setting_up_db.html#step-5-verify-the-database-setup",
    "href": "Spatial SQL/setting_up_db.html#step-5-verify-the-database-setup",
    "title": "Setting up a Spatial Database with Your Local Machine",
    "section": "",
    "text": "You can verify that the database setup is successful by connecting to the database using the psql command-line tool. Run the following command in the terminal:\npsql -d spatial_db\nThis command connects to the ‘spatial_db’ database. You can run SQL queries to verify that the PostGIS extension is enabled and the spatial data is loaded correctly.\nThat’s it! You have successfully set up a spatial database using the PostGIS extension for PostgreSQL. You can now use the database to perform spatial queries and analysis.\n\n\n\nPort: A port is a communication endpoint used to identify specific processes or network services on a computer. PostgreSQL’s default port is 5432. To connect to a database on a different port, you’d use the command:\npsql -h localhost -p 5433 -U postgres -d spatial_db\nUser: The default user for PostgreSQL is postgres. You can specify a different user using the -U flag:\npsql -U myuser -d spatial_db\nHost: The default host for PostgreSQL is localhost. If your database is hosted on a different server, you can specify the host using the -h flag:\npsql -h myhost -d spatial_db",
    "crumbs": [
      "Spatial SQL/index.qmd",
      "Setting up a Spatial Database with Your Local Machine"
    ]
  },
  {
    "objectID": "Archive/index.html",
    "href": "Archive/index.html",
    "title": "Spatial SQL",
    "section": "",
    "text": "Spatial SQL\nThis section includes analysis using spatial SQL. Spatial SQL is an extension of the SQL (Structured Query Language) that is used for managing and manipulating spatial data in a relational database. It includes special functions and operators for dealing with geographical or geometrical data types, such as points, lines, and polygons. This allows for complex queries involving spatial relationships, such as intersection, union, and difference, as well as distance and containment queries. Examples of databases that support spatial SQL include PostgreSQL (with the PostGIS extension), MySQL, and SQLite (with the SpatiaLite extension).\nFor the sake of consistency, I will demonstrate examples using PostGIS and PostgreSQL.\nIn cases, I will also demonstrate the use of Docker. Docker is an open-source platform that automates the deployment, scaling, and management of applications. It uses containerization technology to encapsulate applications and their dependencies into a standardized unit for software development. This containerization aspect makes it possible to run several Docker containers with different applications or services on a single host without them interfering with each other, ensuring that they work uniformly despite differences in environments. Docker is widely used in DevOps practices such as continuous integration and continuous deployment."
  },
  {
    "objectID": "Archive/2-static-images.html",
    "href": "Archive/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "Archive/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "Archive/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Workspace",
    "section": "",
    "text": "Brief Biography\nA dedicated and insightful Master of Urban Planning student with a concentration in Urban Analytics and Climate Resilience at Harvard Graduate School of Design, aiming for completion in May 2024. With a rich background in architecture and finance, I possess a unique blend of design, analytical, and financial skills. My academic and professional journey is driven by a passion for sustainable urban development, climate resilience, and the innovative use of spatial data to inform policy and design.\n\n\nObjective\nThis portfolio showcases my comprehensive skill set, highlighting projects that span topical, analytical, and technical aspects of urban planning and climate resilience. It reflects my commitment to leveraging urban analytics for sustainable and resilient urban futures.\n\n\nSkills Summary\nProficient in Python, R, ArcGIS Pro, Spatial Database Management, Spatial SQL, Spatial Statistics, Remote Sensing, Grasshopper (ENVI-met), InDesign, Machine Learning, and Git. My coursework and professional experiences have equipped me with the tools to tackle complex urban and environmental challenges.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "Topical/index.html",
    "href": "Topical/index.html",
    "title": "Topical",
    "section": "",
    "text": "Topical\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader.",
    "crumbs": [
      "Topical"
    ]
  },
  {
    "objectID": "Topical/bos_uhi.html",
    "href": "Topical/bos_uhi.html",
    "title": "Urban Heat Island Effect",
    "section": "",
    "text": "In this study, I explore the mitigating effects of green and blue spaces on Urban Heat Islands (UHIs) in Boston. This research is driven by the growing concern over UHIs, where urban areas experience higher temperatures than their rural counterparts, exacerbating heatwaves.\nBoston, with its rich mix of historical, commercial, and natural landscapes, serves as an ideal case study for assessing how parks, rivers, and the ocean contribute to cooling urban environments.\nMy findings aim to provide actionable insights for urban planning, emphasizing the critical role of integrating green and blue spaces in urban se ttings to combat the UHI effect.\n\n\n\nfrom IPython.display import display, HTML\n\n# Define the HTML code for the iframe\niframe_html = '&lt;iframe src=\"https://storymaps.arcgis.com/stories/711800e5c4344477b14ebe381dcf7759\" width=\"100%\" height=\"500px\" frameborder=\"0\" allowfullscreen allow=\"geolocation\"&gt;&lt;/iframe&gt;'\n\n# Display the iframe\ndisplay(HTML(iframe_html))\n\n/Users/gigisung/anaconda3/envs/eda/lib/python3.12/site-packages/IPython/core/display.py:431: UserWarning: Consider using IPython.display.IFrame instead\n  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n\n\n\n\n\n\n\n\n\n\nIn the heart of Boston, as in many cities around the world, urban heat islands (UHIs) are a growing concern. These areas can be several degrees hotter than surrounding rural areas because buildings, roads, and pavement absorb and trap heat. This can worsen heatwaves, making them more intense and longer-lasting (Environmental Protection Agency, 2023). The good news is that there’s a natural solution: urban green spaces like parks and blue spaces like rivers and lakes can help cool cities down. Studies have shown that these areas can significantly lower temperatures, providing much-needed relief during hot weather (The Nature Conservancy, 2023).\n\n\n\n\n\nBoston, with its intricate blend of history and modernity, serves as a fascinating urban tapestry that showcases a unique convergence of architectural heritage, bustling commercial hubs, and lush green spaces. As the largest city in New England and one of the oldest cities in the United States, founded in 1630, Boston has evolved into a vibrant urban center that reflects a deliberate balance between development and preservation (Kennedy, 1994). The city’s landscape is marked by a variety of land uses, presenting an intriguing mix of densely populated residential areas, thriving business districts, and significant historical sites.\nCentral to Boston’s urban green infrastructure is the Boston Common, the oldest city park in the United States, established in 1634. Alongside the adjacent Public Garden, it forms the heart of the city’s green spaces, offering residents and visitors alike a verdant escape from the urban environment (Seasholes, 2003). Further enriching Boston’s green landscape is the Emerald Necklace, a 1,100-acre chain of parks linked by parkways and waterways. Designed in the late 19th century by Frederick Law Olmsted, the father of American landscape architecture, this system of parks extends from Boston Common to Franklin Park, weaving through the city and exemplifying urban planning that integrates natural and built environments 1.\nMoreover, Boston’s geographical setting along the Massachusetts Bay, with the Charles River winding through the city, adds a critical blue dimension to its urban fabric. The Charles River Esplanade, an iconic park along the river’s Boston bank, not only provides scenic beauty and recreational opportunities but also plays a vital role in the city’s ecological health. The river itself has undergone significant restoration efforts, transforming it from a polluted waterway to a vibrant aquatic habitat and recreational resource, illustrating successful urban water management (EPA Charles River, 2020) 2.\nThe diversity in Boston’s land use, characterized by its blend of historical sites, modern urban areas, expansive green spaces, and significant water bodies, positions it as an exemplary case study for exploring urban environmental dynamics. The city’s efforts to maintain and enhance its parks and waterfronts reflect a broader commitment to sustainability and resilience in the face of climate change challenges. As such, examining the cooling effects of Boston’s parks and water bodies can provide valuable insights into the role of urban green and blue spaces in mitigating the urban heat island effect, offering lessons that can be applied in other urban contexts.\n\n\n\n\n\nI adopted a methodology that combines Landsat8 satellite imagery and GIS data to assess the relationship between the proximity and size of green and blue spaces and urban temperatures in Boston. By drawing from and adapting the methodology of Park Jong-Hwa & Cho Gi-Hyoug (2016), my study specifically focuses on Land Surface Temperature (LST) as the quantifiable measure of UHIs. Notably, I excluded Digital Elevation Model (DEM) data due to Boston’s relatively flat terrain and instead emphasized the importance of proximity to water bodies. Variables such as Land Use, NDVI, NDBI, and Albedo were controlled to isolate their effects on urban temperatures. My approach involves processing Landsat 8 satellite imagery to derive environmental indicators and reclassifying land use within Boston to understand how urban green and blue spaces impact urban heat.\n\n\nIn this study, adopted was a comprehensive approach to understanding urban heat islands (UHIs) in Boston, employing satellite imagery and Massachusetts GIS databases. By calculating the distances from urban areas to these natural spaces and categorizing their sizes, I was able to assess how proximity to and the size of parks and water bodies correlate with urban temperatures. In other words, I used multi-regression model to explain how environmental and institutional factors contribute to UHI.\nSpecifically, I focused on Land Surface Temperature (LST) as a quantifiable measure of UHIs, drawing on the methodology established by Park, Jong-Hwa, & Cho, Gi-Hyoug (2016) in their study “Influence of park size on the park cooling effect - Focused on Ilsan new town in Korea,” published in the Journal of Korea Planning Association.\nWhile this research is inspired by Park and Cho’s (2016) investigation into the cooling effects of park size in Ilsan, South Korea, there are notable adaptations in this approach to suit the unique urban and geographical context of Boston. Unlike Ilsan, where variations in elevation (DEM - Digital Elevation Model data) play a significant role in the study due to the town’s more pronounced topographical variations, Boston’s relatively flat terrain led to omit DEM as a variable. This decision was based on the understanding that Boston’s elevation changes are not as drastic and are unlikely to significantly impact LST in the manner observed in Ilsan.\nAdditionally, this research expands the scope to include the influence of proximity to water bodies on urban cooling, acknowledging the substantial role that urban blue spaces play alongside green spaces in mitigating heat. In doing so, this study recognize the unique characteristics of Boston’s geography, particularly its adjacency to the Atlantic Ocean and the presence of significant water bodies such as the Charles River. However, I did not account for the size of water bodies in this analysis. The rationale behind this decision stems from the practical consideration that the vast expanse of the Atlantic Ocean and the variability in the size of local water bodies present challenges in quantifying their direct impact on LST using the same methodology applied to urban parks. Instead, the focus on distance from water bodies aims to capture the general cooling influence that proximity to blue spaces has on the urban thermal environment.\n\n\n\nUtilizing satellite imagery and vector resources was instrumental in deriving key environmental indicators such as Land Surface Temperature (LST), Normalized Difference Vegetation Index (NDVI), Normalized Difference Built-up Index (NDBI), and Albedo, alongside mapping the spatial distribution of water bodies and reclassifying land use within the study area.\nThe primary data source was the Landsat 8 satellite, accessible via the Google Earth Engine (GEE) platform, which offers an extensive archive of global satellite imagery. For this study, I focused on imagery collected during the warmer months of May 1 to September 15, 2023, to capture the peak of the UHI effect. The Landsat 8 imagery, specifically the Operational Land Imager (OLI) and Thermal Infrared Sensor (TIRS) data, provided the necessary spectral bands to calculate LST, NDVI, NDBI, and Albedo.\n\n\n\nLST processing Google Earth Engine code script is here.\nLST is crucial for understanding the heat emanating from the Earth’s surface, particularly in urban areas. The calculation of LST from Landsat 8 involves using the thermal infrared sensor (TIRS) bands. Here is a simplified version of the LST calculation process:\n\nConvert Digital Numbers (DN) to Top of Atmosphere (TOA) Radiance:\n\n\nLλ = ML × Qcal + AL\n\n\nLλ: TOA radiance\nML: Band-specific multiplicative rescaling factor (from metadata)\nQcal: Quantized and calibrated standard product pixel values (DN)\nAL: Band-specific additive rescaling factor (from metadata)\n\n\nConvert TOA Radiance to Brightness Temperature:\n\n\nT = K2 / ln((K1 / Lλ) + 1)\n\n\nT: Brightness temperature in Kelvin\nK1, K2: Calibration constants for each TIRS band\nln: Natural logarithm\n\n\nConvert Brightness Temperature to Land Surface Temperature:\n\nThe actual LST calculation requires additional corrections for emissivity, which can be complex and context-specific. A simplified formula does not account for these factors accurately but provides a basis for understanding the process.\n\nThe distribution of LST pixel values. The two peaks indicate two populations, most likely the land and the ocean.\n\n\n\nAlbedo, NDVI, and NDBI processing Google Earth Engine code script is here.\nAlbedo, the measure of reflectance or reflectivity of the Earth’s surface, can be estimated using visible and near-infrared (NIR) bands. For Landsat 8, a simplified formula might look like this:\n\nAlbedo = (0.356B2 + 0.130B4 + 0.373B5 + 0.085B6 + 0.072*B7 - 0.0018) / (0.356 + 0.130 + 0.373 + 0.085 + 0.072)\n\nWhere B2, B4, B5, B6, and B7 are TOA reflectance values for the blue, red, NIR, SWIR1, and SWIR2 bands, respectively.\n\n\n\nNDVI is a widely used index to gauge vegetation health and coverage. It is calculated as:\n\nNDVI = (NIR - Red) / (NIR + Red)\n\nFor Landsat 8, NIR is represented by band 5, and Red by band 4. The NDVI values range from -1 to 1, where higher values indicate healthier vegetation.\n\n\n\nNDBI helps identify built-up areas, contrasting them with natural landscapes. It is calculated as:\n\nNDBI = (SWIR - NIR) / (SWIR + NIR)\n\nFor Landsat 8, SWIR is represented by band 6, and NIR by band 5. Higher NDBI values indicate more built-up areas.\n\n\n\nIn the absence of readily available vector data for water bodies unlike the detailed park datasets, I leveraged satellite imagery. Utilizing the Google Earth Engine (GEE) platform, I first deployed a Landsat 8 image composite for the period between May 1 and September 15, 2023, focusing on true color visualization to accurately identify water bodies alongside urban and green areas. Through supervised classification techniques, I combined manually labeled training data representing urban, green, and water classes to develop a classification model. The classification yielded a detailed land cover map, highlighting the distribution of water bodies, urban areas, and green spaces at a 30-meter resolution. The water pixels were then turned into polygon using ESRI’s “Raster to Polygon” tool. In doing so, I could calculate the distance to water bodies from each grid cell using “Near” geoprocessing tool.\nLand cover classification using supervised classification in GEE. The Script is available here.\nFor land use, this study relied on Massachusetts GIS (MassGIS) data. I took 201 land use codes and reclassified them into 9 categories, which are: ‘Residential_Low’, ‘Residential_Medium’, ‘Residential_High’, ‘Residential_Mixed’, ‘Residential_Vacant’, ‘Commercial’, ‘Industrial’, ‘Open Space’, and ‘Other’\nLand use reclassification python template is available here.\n\n\n\nBy dividing Boston into 30 by 30 meter square grids, we essentially create a matrix of ‘bins’ across the city’s landscape, each bin acting as a container for both raster (e.g., satellite imagery for LST, NDVI, NDBI, and Albedo) and vector (e.g., park polygons, water bodies, land use categories) data. This binning approach allows us to aggregate and analyze environmental data at a fine spatial resolution, corresponding to the Landsat 8 satellite imagery’s resolution. Each square grid, or bin, serves as an individual unit of analysis in our regression modeling.\n\n\n\nFinally, when aggregated to the square grid cells, each factor looks like the following:\nAverage Land Surface Temperature (LST)\n\n\n\nAverage LST\n\n\nThe average LST map displays the spatial distribution of surface heat across Boston. Higher temperatures, depicted in warmer colors, often correlate with urbanized areas lacking vegetative cover. This visual aggregation highlights hotspots within the city, providing a clear representation of UHI effects. We can clearly see the divide in LST average values in between greeneries and built-up areas.\n\n\n\nWhat variables do we want to control?\nLand Use Rationale: Different land use categories (e.g., residential, commercial, industrial) have distinct physical and material characteristics that influence surface temperatures. For example, industrial areas might have more impervious surfaces, leading to higher temperatures compared to residential areas with more vegetation.\nNDVI (Normalized Difference Vegetation Index) Rationale: NDVI measures vegetation health and density, which are directly linked to cooling through evapotranspiration and shading. Higher NDVI values typically correspond to cooler surface temperatures.\nNDBI (Normalized Difference Built-up Index) Rationale: NDBI highlights the extent of built-up areas, which are associated with higher surface temperatures due to materials like concrete and asphalt that retain heat.\nAlbedo Rationale: Albedo represents the reflectivity of surfaces, with higher albedo surfaces reflecting more solar radiation and thus absorbing less heat. Variations in albedo across different urban surfaces can significantly affect local temperatures.",
    "crumbs": [
      "Topical",
      "Urban Heat Island Effect"
    ]
  },
  {
    "objectID": "Topical/bos_uhi.html#storymap",
    "href": "Topical/bos_uhi.html#storymap",
    "title": "Urban Heat Island Effect",
    "section": "",
    "text": "from IPython.display import display, HTML\n\n# Define the HTML code for the iframe\niframe_html = '&lt;iframe src=\"https://storymaps.arcgis.com/stories/711800e5c4344477b14ebe381dcf7759\" width=\"100%\" height=\"500px\" frameborder=\"0\" allowfullscreen allow=\"geolocation\"&gt;&lt;/iframe&gt;'\n\n# Display the iframe\ndisplay(HTML(iframe_html))\n\n/Users/gigisung/anaconda3/envs/eda/lib/python3.12/site-packages/IPython/core/display.py:431: UserWarning: Consider using IPython.display.IFrame instead\n  warnings.warn(\"Consider using IPython.display.IFrame instead\")",
    "crumbs": [
      "Topical",
      "Urban Heat Island Effect"
    ]
  },
  {
    "objectID": "Topical/bos_uhi.html#introduction",
    "href": "Topical/bos_uhi.html#introduction",
    "title": "Urban Heat Island Effect",
    "section": "",
    "text": "In the heart of Boston, as in many cities around the world, urban heat islands (UHIs) are a growing concern. These areas can be several degrees hotter than surrounding rural areas because buildings, roads, and pavement absorb and trap heat. This can worsen heatwaves, making them more intense and longer-lasting (Environmental Protection Agency, 2023). The good news is that there’s a natural solution: urban green spaces like parks and blue spaces like rivers and lakes can help cool cities down. Studies have shown that these areas can significantly lower temperatures, providing much-needed relief during hot weather (The Nature Conservancy, 2023).\n\n\n\n\n\nBoston, with its intricate blend of history and modernity, serves as a fascinating urban tapestry that showcases a unique convergence of architectural heritage, bustling commercial hubs, and lush green spaces. As the largest city in New England and one of the oldest cities in the United States, founded in 1630, Boston has evolved into a vibrant urban center that reflects a deliberate balance between development and preservation (Kennedy, 1994). The city’s landscape is marked by a variety of land uses, presenting an intriguing mix of densely populated residential areas, thriving business districts, and significant historical sites.\nCentral to Boston’s urban green infrastructure is the Boston Common, the oldest city park in the United States, established in 1634. Alongside the adjacent Public Garden, it forms the heart of the city’s green spaces, offering residents and visitors alike a verdant escape from the urban environment (Seasholes, 2003). Further enriching Boston’s green landscape is the Emerald Necklace, a 1,100-acre chain of parks linked by parkways and waterways. Designed in the late 19th century by Frederick Law Olmsted, the father of American landscape architecture, this system of parks extends from Boston Common to Franklin Park, weaving through the city and exemplifying urban planning that integrates natural and built environments 1.\nMoreover, Boston’s geographical setting along the Massachusetts Bay, with the Charles River winding through the city, adds a critical blue dimension to its urban fabric. The Charles River Esplanade, an iconic park along the river’s Boston bank, not only provides scenic beauty and recreational opportunities but also plays a vital role in the city’s ecological health. The river itself has undergone significant restoration efforts, transforming it from a polluted waterway to a vibrant aquatic habitat and recreational resource, illustrating successful urban water management (EPA Charles River, 2020) 2.\nThe diversity in Boston’s land use, characterized by its blend of historical sites, modern urban areas, expansive green spaces, and significant water bodies, positions it as an exemplary case study for exploring urban environmental dynamics. The city’s efforts to maintain and enhance its parks and waterfronts reflect a broader commitment to sustainability and resilience in the face of climate change challenges. As such, examining the cooling effects of Boston’s parks and water bodies can provide valuable insights into the role of urban green and blue spaces in mitigating the urban heat island effect, offering lessons that can be applied in other urban contexts.",
    "crumbs": [
      "Topical",
      "Urban Heat Island Effect"
    ]
  },
  {
    "objectID": "Topical/bos_uhi.html#methodology",
    "href": "Topical/bos_uhi.html#methodology",
    "title": "Urban Heat Island Effect",
    "section": "",
    "text": "I adopted a methodology that combines Landsat8 satellite imagery and GIS data to assess the relationship between the proximity and size of green and blue spaces and urban temperatures in Boston. By drawing from and adapting the methodology of Park Jong-Hwa & Cho Gi-Hyoug (2016), my study specifically focuses on Land Surface Temperature (LST) as the quantifiable measure of UHIs. Notably, I excluded Digital Elevation Model (DEM) data due to Boston’s relatively flat terrain and instead emphasized the importance of proximity to water bodies. Variables such as Land Use, NDVI, NDBI, and Albedo were controlled to isolate their effects on urban temperatures. My approach involves processing Landsat 8 satellite imagery to derive environmental indicators and reclassifying land use within Boston to understand how urban green and blue spaces impact urban heat.\n\n\nIn this study, adopted was a comprehensive approach to understanding urban heat islands (UHIs) in Boston, employing satellite imagery and Massachusetts GIS databases. By calculating the distances from urban areas to these natural spaces and categorizing their sizes, I was able to assess how proximity to and the size of parks and water bodies correlate with urban temperatures. In other words, I used multi-regression model to explain how environmental and institutional factors contribute to UHI.\nSpecifically, I focused on Land Surface Temperature (LST) as a quantifiable measure of UHIs, drawing on the methodology established by Park, Jong-Hwa, & Cho, Gi-Hyoug (2016) in their study “Influence of park size on the park cooling effect - Focused on Ilsan new town in Korea,” published in the Journal of Korea Planning Association.\nWhile this research is inspired by Park and Cho’s (2016) investigation into the cooling effects of park size in Ilsan, South Korea, there are notable adaptations in this approach to suit the unique urban and geographical context of Boston. Unlike Ilsan, where variations in elevation (DEM - Digital Elevation Model data) play a significant role in the study due to the town’s more pronounced topographical variations, Boston’s relatively flat terrain led to omit DEM as a variable. This decision was based on the understanding that Boston’s elevation changes are not as drastic and are unlikely to significantly impact LST in the manner observed in Ilsan.\nAdditionally, this research expands the scope to include the influence of proximity to water bodies on urban cooling, acknowledging the substantial role that urban blue spaces play alongside green spaces in mitigating heat. In doing so, this study recognize the unique characteristics of Boston’s geography, particularly its adjacency to the Atlantic Ocean and the presence of significant water bodies such as the Charles River. However, I did not account for the size of water bodies in this analysis. The rationale behind this decision stems from the practical consideration that the vast expanse of the Atlantic Ocean and the variability in the size of local water bodies present challenges in quantifying their direct impact on LST using the same methodology applied to urban parks. Instead, the focus on distance from water bodies aims to capture the general cooling influence that proximity to blue spaces has on the urban thermal environment.\n\n\n\nUtilizing satellite imagery and vector resources was instrumental in deriving key environmental indicators such as Land Surface Temperature (LST), Normalized Difference Vegetation Index (NDVI), Normalized Difference Built-up Index (NDBI), and Albedo, alongside mapping the spatial distribution of water bodies and reclassifying land use within the study area.\nThe primary data source was the Landsat 8 satellite, accessible via the Google Earth Engine (GEE) platform, which offers an extensive archive of global satellite imagery. For this study, I focused on imagery collected during the warmer months of May 1 to September 15, 2023, to capture the peak of the UHI effect. The Landsat 8 imagery, specifically the Operational Land Imager (OLI) and Thermal Infrared Sensor (TIRS) data, provided the necessary spectral bands to calculate LST, NDVI, NDBI, and Albedo.\n\n\n\nLST processing Google Earth Engine code script is here.\nLST is crucial for understanding the heat emanating from the Earth’s surface, particularly in urban areas. The calculation of LST from Landsat 8 involves using the thermal infrared sensor (TIRS) bands. Here is a simplified version of the LST calculation process:\n\nConvert Digital Numbers (DN) to Top of Atmosphere (TOA) Radiance:\n\n\nLλ = ML × Qcal + AL\n\n\nLλ: TOA radiance\nML: Band-specific multiplicative rescaling factor (from metadata)\nQcal: Quantized and calibrated standard product pixel values (DN)\nAL: Band-specific additive rescaling factor (from metadata)\n\n\nConvert TOA Radiance to Brightness Temperature:\n\n\nT = K2 / ln((K1 / Lλ) + 1)\n\n\nT: Brightness temperature in Kelvin\nK1, K2: Calibration constants for each TIRS band\nln: Natural logarithm\n\n\nConvert Brightness Temperature to Land Surface Temperature:\n\nThe actual LST calculation requires additional corrections for emissivity, which can be complex and context-specific. A simplified formula does not account for these factors accurately but provides a basis for understanding the process.\n\nThe distribution of LST pixel values. The two peaks indicate two populations, most likely the land and the ocean.\n\n\n\nAlbedo, NDVI, and NDBI processing Google Earth Engine code script is here.\nAlbedo, the measure of reflectance or reflectivity of the Earth’s surface, can be estimated using visible and near-infrared (NIR) bands. For Landsat 8, a simplified formula might look like this:\n\nAlbedo = (0.356B2 + 0.130B4 + 0.373B5 + 0.085B6 + 0.072*B7 - 0.0018) / (0.356 + 0.130 + 0.373 + 0.085 + 0.072)\n\nWhere B2, B4, B5, B6, and B7 are TOA reflectance values for the blue, red, NIR, SWIR1, and SWIR2 bands, respectively.\n\n\n\nNDVI is a widely used index to gauge vegetation health and coverage. It is calculated as:\n\nNDVI = (NIR - Red) / (NIR + Red)\n\nFor Landsat 8, NIR is represented by band 5, and Red by band 4. The NDVI values range from -1 to 1, where higher values indicate healthier vegetation.\n\n\n\nNDBI helps identify built-up areas, contrasting them with natural landscapes. It is calculated as:\n\nNDBI = (SWIR - NIR) / (SWIR + NIR)\n\nFor Landsat 8, SWIR is represented by band 6, and NIR by band 5. Higher NDBI values indicate more built-up areas.\n\n\n\nIn the absence of readily available vector data for water bodies unlike the detailed park datasets, I leveraged satellite imagery. Utilizing the Google Earth Engine (GEE) platform, I first deployed a Landsat 8 image composite for the period between May 1 and September 15, 2023, focusing on true color visualization to accurately identify water bodies alongside urban and green areas. Through supervised classification techniques, I combined manually labeled training data representing urban, green, and water classes to develop a classification model. The classification yielded a detailed land cover map, highlighting the distribution of water bodies, urban areas, and green spaces at a 30-meter resolution. The water pixels were then turned into polygon using ESRI’s “Raster to Polygon” tool. In doing so, I could calculate the distance to water bodies from each grid cell using “Near” geoprocessing tool.\nLand cover classification using supervised classification in GEE. The Script is available here.\nFor land use, this study relied on Massachusetts GIS (MassGIS) data. I took 201 land use codes and reclassified them into 9 categories, which are: ‘Residential_Low’, ‘Residential_Medium’, ‘Residential_High’, ‘Residential_Mixed’, ‘Residential_Vacant’, ‘Commercial’, ‘Industrial’, ‘Open Space’, and ‘Other’\nLand use reclassification python template is available here.\n\n\n\nBy dividing Boston into 30 by 30 meter square grids, we essentially create a matrix of ‘bins’ across the city’s landscape, each bin acting as a container for both raster (e.g., satellite imagery for LST, NDVI, NDBI, and Albedo) and vector (e.g., park polygons, water bodies, land use categories) data. This binning approach allows us to aggregate and analyze environmental data at a fine spatial resolution, corresponding to the Landsat 8 satellite imagery’s resolution. Each square grid, or bin, serves as an individual unit of analysis in our regression modeling.\n\n\n\nFinally, when aggregated to the square grid cells, each factor looks like the following:\nAverage Land Surface Temperature (LST)\n\n\n\nAverage LST\n\n\nThe average LST map displays the spatial distribution of surface heat across Boston. Higher temperatures, depicted in warmer colors, often correlate with urbanized areas lacking vegetative cover. This visual aggregation highlights hotspots within the city, providing a clear representation of UHI effects. We can clearly see the divide in LST average values in between greeneries and built-up areas.\n\n\n\nWhat variables do we want to control?\nLand Use Rationale: Different land use categories (e.g., residential, commercial, industrial) have distinct physical and material characteristics that influence surface temperatures. For example, industrial areas might have more impervious surfaces, leading to higher temperatures compared to residential areas with more vegetation.\nNDVI (Normalized Difference Vegetation Index) Rationale: NDVI measures vegetation health and density, which are directly linked to cooling through evapotranspiration and shading. Higher NDVI values typically correspond to cooler surface temperatures.\nNDBI (Normalized Difference Built-up Index) Rationale: NDBI highlights the extent of built-up areas, which are associated with higher surface temperatures due to materials like concrete and asphalt that retain heat.\nAlbedo Rationale: Albedo represents the reflectivity of surfaces, with higher albedo surfaces reflecting more solar radiation and thus absorbing less heat. Variations in albedo across different urban surfaces can significantly affect local temperatures.",
    "crumbs": [
      "Topical",
      "Urban Heat Island Effect"
    ]
  },
  {
    "objectID": "Topical/bos_uhi.html#footnotes",
    "href": "Topical/bos_uhi.html#footnotes",
    "title": "Urban Heat Island Effect",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://storymaps.arcgis.com/stories/7942e99f4f344ca083422b302e03f3ce↩︎\nhttps://www.epa.gov/charlesriver/about-charles-river↩︎",
    "crumbs": [
      "Topical",
      "Urban Heat Island Effect"
    ]
  },
  {
    "objectID": "Work in Progress/index.html",
    "href": "Work in Progress/index.html",
    "title": "Work in Progress",
    "section": "",
    "text": "Work in Progress\nThis section includes\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader.",
    "crumbs": [
      "Work in Progress"
    ]
  }
]